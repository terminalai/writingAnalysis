{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1089791b-0420-4fb1-9b29-8ab602e70eb9",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7f09c-20ed-4226-8bd1-9e5f81b82cb1",
   "metadata": {},
   "source": [
    "Ensure that huggingface is installed via pip and not conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88901be2-4b67-4597-9cd8-b30aebc90da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from transformers import RobertaTokenizerFast, AutoTokenizer\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification\n",
    "from transformers.models.roberta.modeling_roberta import RobertaEncoder\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25a0f3f-f192-408f-92c1-3ceef5b3b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_labels = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66038dc1-bd02-4e48-af22-912902a210ab",
   "metadata": {},
   "source": [
    "Lets import the data labelled by the teacher model. It should be pickled and in the form of a pandas dataframe. We then format the dataframe before passing it into datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00147e68-378f-4cb4-8b3e-e4c6e09525c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 2019 a wave of anti-abortion laws swept thi...</td>\n",
       "      <td>[6.635360240936279, -2.915717124938965, -0.175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But these grabbed the public’s attention in a ...</td>\n",
       "      <td>[-0.4229026734828949, -1.9662591218948364, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Georgia banned abortion after about six weeks ...</td>\n",
       "      <td>[1.8596746921539309, -3.230958938598633, -0.38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ohio, Mississippi, Louisiana and Kentucky did ...</td>\n",
       "      <td>[0.3270220756530761, -2.367975234985352, -1.59...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama went the furthest, banning virtually a...</td>\n",
       "      <td>[6.683916568756104, -2.563556671142578, -0.671...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28359</th>\n",
       "      <td>Jim Justice of West Virginia, a Republican, sa...</td>\n",
       "      <td>[-0.8264712691307068, -2.1442904472351074, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28360</th>\n",
       "      <td>The Biden administration had hoped to avoid sh...</td>\n",
       "      <td>[3.8891327381134033, -3.152361392974853, 1.242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28361</th>\n",
       "      <td>Late last year, the White House persuaded Ariz...</td>\n",
       "      <td>[5.278595924377441, -2.9199347496032715, -0.32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28362</th>\n",
       "      <td>Yet administration officials are less concerne...</td>\n",
       "      <td>[-0.3672137260437011, -2.0636661052703857, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28363</th>\n",
       "      <td>Gene Sperling, who oversees the Biden administ...</td>\n",
       "      <td>[0.5287293195724487, -2.7884507179260254, 0.09...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28364 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      In 2019 a wave of anti-abortion laws swept thi...   \n",
       "1      But these grabbed the public’s attention in a ...   \n",
       "2      Georgia banned abortion after about six weeks ...   \n",
       "3      Ohio, Mississippi, Louisiana and Kentucky did ...   \n",
       "4      Alabama went the furthest, banning virtually a...   \n",
       "...                                                  ...   \n",
       "28359  Jim Justice of West Virginia, a Republican, sa...   \n",
       "28360  The Biden administration had hoped to avoid sh...   \n",
       "28361  Late last year, the White House persuaded Ariz...   \n",
       "28362  Yet administration officials are less concerne...   \n",
       "28363  Gene Sperling, who oversees the Biden administ...   \n",
       "\n",
       "                                                  labels  \n",
       "0      [6.635360240936279, -2.915717124938965, -0.175...  \n",
       "1      [-0.4229026734828949, -1.9662591218948364, 0.2...  \n",
       "2      [1.8596746921539309, -3.230958938598633, -0.38...  \n",
       "3      [0.3270220756530761, -2.367975234985352, -1.59...  \n",
       "4      [6.683916568756104, -2.563556671142578, -0.671...  \n",
       "...                                                  ...  \n",
       "28359  [-0.8264712691307068, -2.1442904472351074, -0....  \n",
       "28360  [3.8891327381134033, -3.152361392974853, 1.242...  \n",
       "28361  [5.278595924377441, -2.9199347496032715, -0.32...  \n",
       "28362  [-0.3672137260437011, -2.0636661052703857, 0.5...  \n",
       "28363  [0.5287293195724487, -2.7884507179260254, 0.09...  \n",
       "\n",
       "[28364 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_path = r\"teacher_labels_0.pkl\"\n",
    "#df = pd.read_pickle(data_path)\n",
    "data_path = r\"data/pseudolab.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "df[\"labels\"] = df.iloc[:,1:-1].apply(lambda x: x.to_list(), axis=1)\n",
    "df = df.drop(columns=df.columns[1:-1])\n",
    "df = df.rename(columns={\"sentence\":\"text\"})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f1257-49f0-48d9-8286-0c6978318c77",
   "metadata": {},
   "source": [
    "We load the df into a dataset, where the labels take the same form of the multi label classification problem, except with float values ranging from 0-1 instead. We don't have to customise the tokenizer to suit our needs as both models are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c6ae3a-21f7-4044-98e7-71103a1cfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model_checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_checkpoint,\n",
    "                                          problem_type=\"multi_label_classification\",\n",
    "                                          use_fast=True, max_length=512)\n",
    "\n",
    "#tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_model_checkpoint,\n",
    "#                                                 problem_type=\"multi_label_classification\",\n",
    "#                                                 use_fast=True)\n",
    "\n",
    "def tokenize_and_encode(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d92408f-41d2-4bca-8366-1dc46708a19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273c4fc83612415a96423eaca0c8db53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d77befde0744da0b53e5e30bd08030f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels'],\n",
       "        num_rows: 19854\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels'],\n",
       "        num_rows: 8510\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "dataset = dataset.map(tokenize_and_encode, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "'''\n",
    "features = dataset.features.copy()\n",
    "features[\"labels\"] = Sequence(feature=Value(\"float32\", id=None), length=-1, id=None)\n",
    "dataset.cast_(features)\n",
    "'''\n",
    "\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e82c5-feac-4bdd-a456-3e69265b6bee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TRAINER USING SOFT LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6744b4-072c-4bb9-8ed9-a7c860f6a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NoisyTrainingArguments(TrainingArguments):\n",
    "    temperature: float = field(default=2.0, metadata={\"help\": \"Temperature for the softmax temperature.\"})\n",
    "    alpha_ce: float = field(default=0.5, metadata={\"help\":\"Linear weight for the distillation loss. Must be >=0.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f087d0a5-8809-413d-ab7d-27fd14bfb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyStudentTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # extract required parameters from our subclassed training arguments\n",
    "        temperature = self.args.temperature\n",
    "        alpha_ce = self.args.alpha_ce\n",
    "        \n",
    "        # get the labels of the input\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # get the outputs of the model forward pass\n",
    "        '''\n",
    "        attention_mask = inputs.attention_mask\n",
    "        input_ids = inputs.input_ids\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        '''\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # sanity check\n",
    "        assert logits.size() == labels.size()\n",
    "        \n",
    "        # Kullback-Leibler Divergence loss (cross entropy)\n",
    "        self.ce_loss_func = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        \n",
    "        # compute KLDiv loss and multiply by alpha value\n",
    "        loss_ce = self.ce_loss_func(nn.functional.log_softmax(logits / temperature, dim=-1),\n",
    "                                    nn.functional.softmax(labels / temperature, dim=-1)) * (temperature ** 2)\n",
    "        loss = alpha_ce * loss_ce\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee783c36-4840-490c-a88e-1299d58bcc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaConfig(RobertaConfig):\n",
    "    def __init__(self, layerdrop=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layerdrop = layerdrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "398fe3b5-b9d9-4ad5-ab8c-1344412b925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaEncoder(RobertaEncoder):\n",
    "    # override the forward pass. Essentially mostly the same as the original code.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.config.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "                past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    if use_cache:\n",
    "                        logger.warning(\n",
    "                            \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                        )\n",
    "                        use_cache = False\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(layer_module),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = layer_module(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                        past_key_value,\n",
    "                        output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "                if use_cache:\n",
    "                    next_decoder_cache += (layer_outputs[-1],)\n",
    "                \n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "class NoisyRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config: NoisyRobertaConfig):\n",
    "        super().__init__(config)\n",
    "        self.roberta.encoder = NoisyRobertaEncoder(config)\n",
    "        self.roberta.post_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825cae0d-c16e-4245-b897-7bb2c144b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = NoisyRobertaConfig(layerdrop=0,\n",
    "                            num_labels=num_labels,\n",
    "                            problem_type=\"multi_label_classification\")\n",
    "\n",
    "model = NoisyRobertaForSequenceClassification(config)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.roberta.embeddings.position_embeddings.weight.requires_grad = False\n",
    "model.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1de4927-e6a9-4f4b-b2d2-a1c5692f40a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = NoisyTrainingArguments(\n",
    "    output_dir=\"models_gitignored/roberta_noisy/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,    \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True)\n",
    "\n",
    "trainer = NoisyStudentTrainer(model=model,\n",
    "                              args=args, \n",
    "                              tokenizer=tokenizer,\n",
    "#                              data_collator=data_collator,\n",
    "                              train_dataset=dataset[\"train\"], \n",
    "                              eval_dataset=dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c015b7f6-334d-4db1-a8aa-ea27f4a7bfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipy\\anaconda3\\envs\\cspy\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19854\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='9928' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   7/9928 00:05 < 2:54:19, 0.95 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cspy\\lib\\site-packages\\transformers\\trainer.py:1465\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1463\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m scale_before \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scale_after\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1465\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[0;32m   1468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cspy\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cspy\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cspy\\lib\\site-packages\\transformers\\optimization.py:359\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    355\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    361\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6243cf9b-3646-46ce-8f3b-1a6e61bd42d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# TRAINER WITH TEACHER MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e88f73-3827-4bab-83d6-ee21689d6db9",
   "metadata": {},
   "source": [
    "Here we subclass the TrainingArguments to inject our own parameters required for soft label loss computation. We follow the same format as official huggingface implementation referencing [seq2seq training arguments](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args_seq2seq.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff526b7-d7da-4ed8-af0a-41054e000dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NoisyTrainingArguments(TrainingArguments):\n",
    "    temperature: float = field(default=2.0, metadata={\"help\": \"Temperature for the softmax temperature.\"})\n",
    "    alpha_ce: float = field(default=0.5, metadata={\"help\":\"Linear weight for the distillation loss. Must be >=0.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9b6bc-7f44-474b-a5dc-b9d508c5b68f",
   "metadata": {},
   "source": [
    "We subclass the trainer and define our own compute_loss for soft labels. Much of this is based off the original work of [distil models](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff82590-2b61-434a-ad3f-d77b84b9d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyStudentTrainer(Trainer):\n",
    "    def __init__(self, teacher_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        \n",
    "        # move the teacher model to the same device as the student model\n",
    "        self._move_model_to_device(self.teacher_model, self.model.device)\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # extract required parameters from our subclassed training arguments\n",
    "        temperature = self.args.temperature\n",
    "        alpha_ce = self.args.alpha_ce\n",
    "        \n",
    "        print(inputs)\n",
    "        \n",
    "        student_outputs = model(**inputs)\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher_model(**inputs)\n",
    "        t_logits = teacher_output.logits\n",
    "        s_logits = student_outputs.logits\n",
    "        \n",
    "        attention_mask = inputs.get(\"attention_mask\")\n",
    "        \n",
    "        # sanity check\n",
    "        assert t_logits.size() == s_logits.size()\n",
    "        \n",
    "        # Kullback-Leibler Divergence loss (cross entropy)\n",
    "        self.ce_loss_func = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        \n",
    "        # compute KLDiv loss and multiply by alpha value\n",
    "        loss_ce = self.ce_loss_func(nn.functional.log_softmax(s_logits / temperature, dim=-1),\n",
    "                                    nn.functional.softmax(t_logits / temperature, dim=-1)) * (temperature ** 2)\n",
    "        loss = alpha_ce * loss_ce\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a44ad-e1f1-469a-bfd8-7805abc45280",
   "metadata": {},
   "source": [
    "Subclass RobertaConfig to include our own parameters relevant to layerdrop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86972ebb-63a4-48fd-b0c0-0501bb262b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaConfig(RobertaConfig):\n",
    "    def __init__(self, layerdrop=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layerdrop = layerdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae600e3a-f78b-496a-9e5b-1d5f022793d7",
   "metadata": {},
   "source": [
    "Subclass the encoder layers in roberta and implement layerdrop. Similar to BERT, RobertaModel can behave as an encoder and decoder, so we only have to subclass the single RobertaEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3cf537-caae-4cc4-b0e6-6683445a8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaEncoder(RobertaEncoder):\n",
    "    # override the forward pass. Essentially mostly the same as the original code.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.config.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "                past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    if use_cache:\n",
    "                        logger.warning(\n",
    "                            \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                        )\n",
    "                        use_cache = False\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(layer_module),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = layer_module(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                        past_key_value,\n",
    "                        output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "                if use_cache:\n",
    "                    next_decoder_cache += (layer_outputs[-1],)\n",
    "                \n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "    \n",
    "class NoisyRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config: NoisyRobertaConfig):\n",
    "        super().__init__(config)\n",
    "        self.roberta.encoder = NoisyRobertaEncoder(config)\n",
    "        self.roberta.post_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694ad45-1efc-4887-8f1d-7584314a638c",
   "metadata": {},
   "source": [
    "We also freeze the position and token type weights in roBERTa, I don't know why but the original authors of the distil models did that and I assume there's a good reason for doing so. The output of model is quite large so it is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6c1f6-36d9-424f-b845-0b08d6f7c64d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = NoisyRobertaConfig(layerdrop=0, num_labels=num_labels, problem_type=\"multi_label_classification\", max_length=512)\n",
    "teacher_model = RobertaForSequenceClassification.from_pretrained(r\"models_gitignored/roberta-base-finetuned-sentence-classification/checkpoint-75756/\", num_labels=num_labels, problem_type=\"multi_label_classification\").to(device)\n",
    "model = NoisyRobertaForSequenceClassification(config).to(device)\n",
    "\n",
    "model.roberta.embeddings.position_embeddings.weight.requires_grad = False\n",
    "model.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d30f5-5c42-41d5-ac9d-e4bb25462ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d99385-792f-4af5-aa06-06dddee39e8d",
   "metadata": {},
   "source": [
    "We prayge that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e54fe9-9ba0-41a8-9ec2-97c3a79e6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = NoisyTrainingArguments(\n",
    "    output_dir=\"models_gitignored/NoisyRoberta/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,    \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True)\n",
    "\n",
    "trainer = NoisyStudentTrainer(model=model,\n",
    "                              teacher_model=teacher_model,\n",
    "                              args=args, \n",
    "                              tokenizer=tokenizer,\n",
    "#                              data_collator=data_collator,\n",
    "                              train_dataset=dataset[\"train\"], \n",
    "                              eval_dataset=dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35da75c-bb6e-42fc-aba5-aed7146f3ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a8d88-7918-47de-9878-e945e814eef4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# REFERENCE LINKS:\n",
    "[HOW NICE A BLOG POST](https://www.philschmid.de/knowledge-distillation-bert-transformers) \\\n",
    "[HF FORUM POST DISTILLATION WITH TRAINER](https://discuss.huggingface.co/t/does-it-make-sense-to-train-distilbert-from-scratch-in-a-new-corpus/3503/2) \\\n",
    "[HF FORUM POST MULTI_LABEL_CLASSIFICATION DATASETS](https://discuss.huggingface.co/t/dataset-label-format-for-multi-label-text-classification/14998/3) \\\n",
    "[HF GITHUB VOCAB SIZE SAVED MY LIFE WHEN STUPID CUDA ASSERTION ERRORS WITH 0 INFORMATION HAPPENED WITH NN.EMBEDDINGS](https://github.com/huggingface/transformers/issues/237) \\\n",
    "[BART GITHUB SOURCE CODE CTRL+F LAYERDROP](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bart/modeling_bart.py) \\\n",
    "[ROBERTA GITHUB SOURCE CODE](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py) \\\n",
    "[DEVICE SIDE ASSERT TRIGGERED ERROR (TL;DR JUST RESTART KERNEL)](https://stackoverflow.com/questions/68166721/cuda-error-device-side-assert-triggered-on-colab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
