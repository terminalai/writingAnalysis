{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1089791b-0420-4fb1-9b29-8ab602e70eb9",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7f09c-20ed-4226-8bd1-9e5f81b82cb1",
   "metadata": {},
   "source": [
    "Ensure that huggingface is installed via pip and not conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88901be2-4b67-4597-9cd8-b30aebc90da0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/weipyn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from transformers import RobertaTokenizerFast, AutoTokenizer\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification\n",
    "from transformers.models.roberta.modeling_roberta import RobertaEncoder\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import copy\n",
    "from pympler import asizeof\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25a0f3f-f192-408f-92c1-3ceef5b3b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_labels = 7\n",
    "num_epochs = 3\n",
    "iterations = 3\n",
    "layerdrop = 0.7\n",
    "load_saved_teacher_labels = True\n",
    "init_teacher_model_path = r\"models_gitignored/roberta-base-finetuned-sentence-classification/checkpoint-75756\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66038dc1-bd02-4e48-af22-912902a210ab",
   "metadata": {},
   "source": [
    "Lets import the data to be labelled by the teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d197a73-485b-4824-8b08-b7c78be730fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Carrie Lam, the chief executive, announced tha...\n",
       "1         The airline expressed its condolences to the r...\n",
       "2         Russian forces had given a predawn deadline fo...\n",
       "3         President Biden’s nominee to the Supreme Court...\n",
       "4         Russian forces had given a predawn deadline fo...\n",
       "                                ...                        \n",
       "149656     if I'm not sure what college I want to attend...\n",
       "149657     seeking multiple opinions before making a har...\n",
       "149658    it is better to seek multiple opinions instead...\n",
       "149659    The impact of asking people to help you make a...\n",
       "149660    there are many other reasons one might want to...\n",
       "Length: 149661, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths = [r\"title-data/comprehensive_data.csv\", r\"datagen/nytimes/pseudo.csv\",\n",
    "              r\"data/feedback_prize/train.csv\"]\n",
    "texts = list()\n",
    "for path in data_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    texts.append(df.filter(regex=(r\".*text.*\")).iloc[:,0])\n",
    "texts_series = pd.concat(texts).reset_index(drop=True)\n",
    "texts_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f1257-49f0-48d9-8286-0c6978318c77",
   "metadata": {},
   "source": [
    "We pass the df into `pseudoLabel()` which returns a dataset, where the labels take the same form of the multi label classification problem, except with float values ranging from 0-1 instead. We don't have to customise the tokenizer to suit our needs as both models are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c6ae3a-21f7-4044-98e7-71103a1cfeb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768, padding_idx=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_model_checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_checkpoint,\n",
    "                                          problem_type=\"multi_label_classification\",\n",
    "                                          use_fast=True, max_length=128)\n",
    "\n",
    "init_teacher_model = RobertaForSequenceClassification.from_pretrained(init_teacher_model_path).to(device)\n",
    "init_teacher_model.resize_token_embeddings(len(tokenizer)) # this fixes the embedding problem\n",
    "#init_teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0899831c-e6bb-4fa2-8aa8-8c0056591ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def pseudoLabel(text, tokenizer, model):    \n",
    "    sentences = text.apply(sentence_tokenizer.tokenize).explode()\n",
    "    print(f\"{len(sentences)} sentences split\")\n",
    "    \n",
    "    def tokenize_and_encode(examples):\n",
    "        return tokenizer(examples['0'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(sentences))\n",
    "    dataset = dataset.map(tokenize_and_encode, batched=True, remove_columns=[\"0\"])\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    \n",
    "    def model_prediction(examples):\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=examples['input_ids'].to(model.device),\n",
    "                            attention_mask=examples['attention_mask'].to(model.device))\n",
    "        return {\"labels\":outputs.logits.cpu().detach().numpy()}\n",
    "    \n",
    "    dataset = dataset.map(model_prediction, batched=True, batch_size=100)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286103b3-b600-4164-8daa-012d313ffc9f",
   "metadata": {},
   "source": [
    "Pickle and dump information just in case we have to restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd9a00d7-ffb3-4a74-a4d7-3f4e6b5ffe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_saved_teacher_labels:\n",
    "    teacher_labelled_dataset = pseudoLabel(texts_series, tokenizer, init_teacher_model)\n",
    "    teacher_labelled_dataset\n",
    "    with open(r\"data/init_teacher_labelled_dataset\", \"ab\") as f:\n",
    "        pickle.dump(teacher_labelled_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10f28e-7936-445f-83cb-e4ff16b287c5",
   "metadata": {},
   "source": [
    "Loading from saved pickle file if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4813e3cc-51f5-4e11-a886-f609ba597958",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_saved_teacher_labels:\n",
    "    with open(r\"data/init_teacher_labelled_dataset\", \"rb\") as f:\n",
    "        teacher_labelled_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e442a-f263-4406-a351-dc85f28b30cc",
   "metadata": {},
   "source": [
    "Split the dataset. We realistically can't use all the data it takes 6 hours to train a single iteration of model and we do not have that kind of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d92408f-41d2-4bca-8366-1dc46708a19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 44743\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 19176\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_dataset = teacher_labelled_dataset.train_test_split(test_size=0.85)\n",
    "teacher_labelled_dataset = holdout_dataset['train'].train_test_split(test_size=0.3)\n",
    "teacher_labelled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe712bd-8399-4486-8c1c-3c260c7d4b6b",
   "metadata": {},
   "source": [
    "Throw in compute_metrics() to determine model performance at the end of every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fac429-af22-4ada-8b89-8e2dad571daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric_acc = load_metric(\"accuracy\")\n",
    "    metric_prec = load_metric(\"precision\")\n",
    "    metric_recall = load_metric(\"recall\")\n",
    "    metric_f1 = load_metric(\"f1\")\n",
    "    \n",
    "    # the logits aren't technically logits since the model head is a sequence classification head so its actually softmaxed probabilities\n",
    "    student_preds, teacher_logits = eval_preds\n",
    "    # since the labels themselves are also logits, we assume the teacher model is superior and that the highest probability teacher logit is the label\n",
    "    labels = np.argmax(nn.functional.softmax(input=torch.tensor(teacher_logits), dim=-1), axis=-1)\n",
    "    predictions = np.argmax(student_preds, axis=-1)\n",
    "    \n",
    "    acc = metric_acc.compute(predictions=predictions, references=labels)\n",
    "    prec = metric_prec.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    recall = metric_recall.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    kappa = cohen_kappa_score(predictions, labels)\n",
    "\n",
    "    return {\"accuracy\": acc['accuracy'], \"precision\": prec['precision'],\n",
    "            \"recall\": recall['recall'], \"f1\": f1['f1'], \"kappa\": kappa}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e82c5-feac-4bdd-a456-3e69265b6bee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TRAINER USING SOFT LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0178df-3338-420b-88c2-437e65838af2",
   "metadata": {},
   "source": [
    "Here we subclass the TrainingArguments to inject our own parameters required for soft label loss computation. We follow the same format as official huggingface implementation referencing [seq2seq training arguments](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args_seq2seq.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c6744b4-072c-4bb9-8ed9-a7c860f6a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NoisyTrainingArguments(TrainingArguments):\n",
    "    temperature: float = field(default=2.0, metadata={\"help\": \"Temperature for the softmax temperature.\"})\n",
    "    alpha_ce: float = field(default=0.5, metadata={\"help\":\"Linear weight for the distillation loss. Must be >=0.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4650457-ef98-4449-afa6-57aa90a0f0c6",
   "metadata": {},
   "source": [
    "We subclass the trainer and define our own compute_loss for soft labels. Much of this is based off the original work of [distil models](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f087d0a5-8809-413d-ab7d-27fd14bfb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyStudentTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # extract required parameters from our subclassed training arguments\n",
    "        temperature = self.args.temperature\n",
    "        alpha_ce = self.args.alpha_ce\n",
    "        \n",
    "        # get the labels of the input\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # get the outputs of the model forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # sanity check\n",
    "        assert logits.size() == labels.size()\n",
    "        \n",
    "        # Kullback-Leibler Divergence loss (cross entropy)\n",
    "        self.ce_loss_func = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        \n",
    "        # compute KLDiv loss and multiply by alpha value\n",
    "        loss_ce = self.ce_loss_func(nn.functional.log_softmax(logits / temperature, dim=-1),\n",
    "                                    nn.functional.softmax(labels / temperature, dim=-1)) * (temperature ** 2)\n",
    "        loss = alpha_ce * loss_ce\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a3303-1cad-4636-bd2f-3257967a126e",
   "metadata": {},
   "source": [
    "Subclass RobertaConfig to include our own parameters relevant to layerdrop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee783c36-4840-490c-a88e-1299d58bcc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaConfig(RobertaConfig):\n",
    "    def __init__(self, layerdrop=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layerdrop = layerdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3588a6-0d7a-4874-ab9c-1579c7ebfcf3",
   "metadata": {},
   "source": [
    "Subclass the encoder layers in roberta and implement layerdrop. Similar to BERT, RobertaModel can behave as an encoder and decoder, so we only have to subclass the single RobertaEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "398fe3b5-b9d9-4ad5-ab8c-1344412b925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaEncoder(RobertaEncoder):\n",
    "    # override the forward pass. Essentially mostly the same as the original code.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.config.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "                past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    if use_cache:\n",
    "                        logger.warning(\n",
    "                            \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                        )\n",
    "                        use_cache = False\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(layer_module),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = layer_module(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                        past_key_value,\n",
    "                        output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "                if use_cache:\n",
    "                    next_decoder_cache += (layer_outputs[-1],)\n",
    "                \n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "class NoisyRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config: NoisyRobertaConfig):\n",
    "        super().__init__(config)\n",
    "        self.roberta.encoder = NoisyRobertaEncoder(config)\n",
    "        self.roberta.post_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd27bf3-af5f-419c-82d4-d74f291f4f6b",
   "metadata": {},
   "source": [
    "We also freeze the position and token type weights in roBERTa, I don't know why but the original authors of the distil models did that and I assume there's a good reason for doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "825cae0d-c16e-4245-b897-7bb2c144b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "/home/weipyn/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 44743\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16779\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16779' max='16779' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16779/16779 26:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.926100</td>\n",
       "      <td>1.060842</td>\n",
       "      <td>0.646746</td>\n",
       "      <td>0.599289</td>\n",
       "      <td>0.646746</td>\n",
       "      <td>0.577421</td>\n",
       "      <td>0.290225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.716200</td>\n",
       "      <td>0.778553</td>\n",
       "      <td>0.712975</td>\n",
       "      <td>0.672029</td>\n",
       "      <td>0.712975</td>\n",
       "      <td>0.689004</td>\n",
       "      <td>0.506632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.643900</td>\n",
       "      <td>0.701761</td>\n",
       "      <td>0.730705</td>\n",
       "      <td>0.697817</td>\n",
       "      <td>0.730705</td>\n",
       "      <td>0.700528</td>\n",
       "      <td>0.521297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19176\n",
      "  Batch size = 8\n",
      "/home/weipyn/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-5593\n",
      "Configuration saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-5593/config.json\n",
      "Model weights saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-5593/pytorch_model.bin\n",
      "tokenizer config file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-5593/tokenizer_config.json\n",
      "Special tokens file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-5593/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19176\n",
      "  Batch size = 8\n",
      "/home/weipyn/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-11186\n",
      "Configuration saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-11186/config.json\n",
      "Model weights saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-11186/pytorch_model.bin\n",
      "tokenizer config file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-11186/tokenizer_config.json\n",
      "Special tokens file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-11186/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19176\n",
      "  Batch size = 8\n",
      "/home/weipyn/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-16779\n",
      "Configuration saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-16779/config.json\n",
      "Model weights saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-16779/pytorch_model.bin\n",
      "tokenizer config file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-16779/tokenizer_config.json\n",
      "Special tokens file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-16779/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from models_gitignored/roberta_noisy_layerdrop0.7_iter0/checkpoint-16779 (score: 0.7017609477043152).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426133 sentences split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bc003f79eb4696a5e2a73e97bd86f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/427 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5463344882f48d6b02eaec48123d873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4262 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "/home/weipyn/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 74573\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27966\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27966' max='27966' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27966/27966 44:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.133940</td>\n",
       "      <td>0.905350</td>\n",
       "      <td>0.913706</td>\n",
       "      <td>0.905350</td>\n",
       "      <td>0.906486</td>\n",
       "      <td>0.816362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>0.065125</td>\n",
       "      <td>0.924312</td>\n",
       "      <td>0.925987</td>\n",
       "      <td>0.924312</td>\n",
       "      <td>0.922460</td>\n",
       "      <td>0.847400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.054133</td>\n",
       "      <td>0.930131</td>\n",
       "      <td>0.932369</td>\n",
       "      <td>0.930131</td>\n",
       "      <td>0.928936</td>\n",
       "      <td>0.861033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31960\n",
      "  Batch size = 8\n",
      "/home/weipyn/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-9322\n",
      "Configuration saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-9322/config.json\n",
      "Model weights saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-9322/pytorch_model.bin\n",
      "tokenizer config file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-9322/tokenizer_config.json\n",
      "Special tokens file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-9322/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31960\n",
      "  Batch size = 8\n",
      "/home/weipyn/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-18644\n",
      "Configuration saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-18644/config.json\n",
      "Model weights saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-18644/pytorch_model.bin\n",
      "tokenizer config file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-18644/tokenizer_config.json\n",
      "Special tokens file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-18644/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31960\n",
      "  Batch size = 8\n",
      "/home/weipyn/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-27966\n",
      "Configuration saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-27966/config.json\n",
      "Model weights saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-27966/pytorch_model.bin\n",
      "tokenizer config file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-27966/tokenizer_config.json\n",
      "Special tokens file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-27966/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from models_gitignored/roberta_noisy_layerdrop0.7_iter1/checkpoint-27966 (score: 0.05413306504487991).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426133 sentences split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d09f56b99c4d94af585c26a10f99a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/427 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762b16b5e98a4b71acf1cf8ad7fdbac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4262 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "/home/weipyn/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 74573\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27966\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27966' max='27966' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27966/27966 44:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.150988</td>\n",
       "      <td>0.857196</td>\n",
       "      <td>0.907644</td>\n",
       "      <td>0.857196</td>\n",
       "      <td>0.867766</td>\n",
       "      <td>0.748458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.058591</td>\n",
       "      <td>0.926220</td>\n",
       "      <td>0.933111</td>\n",
       "      <td>0.926220</td>\n",
       "      <td>0.924010</td>\n",
       "      <td>0.850109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.026404</td>\n",
       "      <td>0.952096</td>\n",
       "      <td>0.955133</td>\n",
       "      <td>0.952096</td>\n",
       "      <td>0.951879</td>\n",
       "      <td>0.901645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31960\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-9322\n",
      "Configuration saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-9322/config.json\n",
      "Model weights saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-9322/pytorch_model.bin\n",
      "tokenizer config file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-9322/tokenizer_config.json\n",
      "Special tokens file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-9322/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31960\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-18644\n",
      "Configuration saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-18644/config.json\n",
      "Model weights saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-18644/pytorch_model.bin\n",
      "tokenizer config file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-18644/tokenizer_config.json\n",
      "Special tokens file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-18644/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31960\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-27966\n",
      "Configuration saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-27966/config.json\n",
      "Model weights saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-27966/pytorch_model.bin\n",
      "tokenizer config file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-27966/tokenizer_config.json\n",
      "Special tokens file saved in models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-27966/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from models_gitignored/roberta_noisy_layerdrop0.7_iter2/checkpoint-27966 (score: 0.02640409767627716).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426133 sentences split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72318280f3b041f8bd30e55b81aeccca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/427 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a684973b4d274ba592b4411b89635fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4262 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    # train the model\n",
    "    config = NoisyRobertaConfig(layerdrop=layerdrop,\n",
    "                                num_labels=num_labels,\n",
    "                                problem_type=\"multi_label_classification\")\n",
    "\n",
    "    model = NoisyRobertaForSequenceClassification(config).to(device)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer)) # this fixes the embedding problem\n",
    "    model.roberta.embeddings.position_embeddings.weight.requires_grad = False\n",
    "    model.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    save_dir = f\"roberta_noisy_layerdrop{layerdrop}_iter{i}\"\n",
    "    args = NoisyTrainingArguments(\n",
    "        output_dir=f\"models_gitignored/{save_dir}/\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,    \n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True)\n",
    "\n",
    "    trainer = NoisyStudentTrainer(model=model,\n",
    "                                  args=args, \n",
    "                                  tokenizer=tokenizer,\n",
    "                                  train_dataset=teacher_labelled_dataset[\"train\"], \n",
    "                                  eval_dataset=teacher_labelled_dataset[\"test\"],\n",
    "                                  compute_metrics=compute_metrics)\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    with open(f\"logs/roberta_noisy/{save_dir}.txt\", \"w\") as fout:\n",
    "        for obj in trainer.state.log_history:\n",
    "            print(obj, file=fout)\n",
    "    \n",
    "    # use the best model for pseudolabelling again\n",
    "    teacher_labelled_dataset = pseudoLabel(texts_series, tokenizer, trainer.model)\n",
    "    with open(f\"data/teacher_labelled_dataset_layerdrop{layerdrop}_iter{i}\", \"ab\") as f:\n",
    "        pickle.dump(teacher_labelled_dataset, f)    \n",
    "    holdout_dataset = teacher_labelled_dataset.train_test_split(test_size=0.75)\n",
    "    teacher_labelled_dataset = holdout_dataset['train'].train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110231e-00de-444a-a069-e4b4997f6520",
   "metadata": {},
   "source": [
    "# Pruning the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a7a01-6971-45d0-87fd-42e9e82e305a",
   "metadata": {},
   "source": [
    "Following the layerdrop paper, we prune using the every other strategy, defined by the congruence relation d$\\equiv$0(mod[$\\frac{1}{p}$]). Which is a fancy way of saying drop layer at depth d if d=k($\\frac{1}{p}$) for some k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13e72847-6b57-4913-afed-7a79eb0ecc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruneLayers(model, p):  # must pass in the full model\n",
    "    oldModuleList = model.roberta.encoder.layer\n",
    "    newModuleList = nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keeping the relevant layers.\n",
    "    for d in range(0, len(oldModuleList)):\n",
    "        if d % (1/p) == 0: continue\n",
    "        newModuleList.append(oldModuleList[i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.roberta.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3996ca1-5752-47e9-b8a1-d663c2e246c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prunedModel = pruneLayers(trainer.model, layerdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "544b0d28-f363-4e92-bb85-22000e3a0e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "args = NoisyTrainingArguments(\n",
    "    output_dir=f\"models_gitignored/{save_dir}/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,    \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True)\n",
    "\n",
    "prunedTrainer = NoisyStudentTrainer(model=prunedModel,\n",
    "                                  args=args, \n",
    "                                  tokenizer=tokenizer,\n",
    "                                  train_dataset=teacher_labelled_dataset[\"train\"], \n",
    "                                  eval_dataset=teacher_labelled_dataset[\"test\"],\n",
    "                                  compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0afe773a-cab4-4f3a-b4f7-2edbcd583fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 383519\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 42614\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r\"data/init_teacher_labelled_dataset\", \"rb\") as f:\n",
    "    eval_labelled_dataset = pickle.load(f)\n",
    "eval_labelled_dataset = eval_labelled_dataset.train_test_split(test_size=0.1)\n",
    "eval_labelled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa1472aa-3f04-47e8-83c1-93a41c78398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42614\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5327' max='5327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5327/5327 02:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weipyn/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.046757459640503,\n",
       " 'eval_accuracy': 0.6996526962969916,\n",
       " 'eval_precision': 0.6731825324243581,\n",
       " 'eval_recall': 0.6996526962969916,\n",
       " 'eval_f1': 0.6802165883555797,\n",
       " 'eval_kappa': 0.4964696291856796,\n",
       " 'eval_runtime': 160.8954,\n",
       " 'eval_samples_per_second': 264.855,\n",
       " 'eval_steps_per_second': 33.108}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prunedTrainer.evaluate(eval_labelled_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eabffbad-88ef-4daf-a12b-b395190329e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42614\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5327' max='5327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5327/5327 02:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weipyn/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8135748505592346,\n",
       " 'eval_accuracy': 0.7065518374243206,\n",
       " 'eval_precision': 0.668486859823052,\n",
       " 'eval_recall': 0.7065518374243206,\n",
       " 'eval_f1': 0.6745953287596376,\n",
       " 'eval_kappa': 0.46453323665763047,\n",
       " 'eval_runtime': 173.5557,\n",
       " 'eval_samples_per_second': 245.535,\n",
       " 'eval_steps_per_second': 30.693,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_labelled_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fa10f04-1448-48fa-883a-d386f1e1be5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110328"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asizeof.asizeof(prunedTrainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b959f638-e8d1-4612-8f1e-66c7fd48b021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701840"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asizeof.asizeof(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e95541cd-783e-4eb2-8e01-c58a4e87b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_eval = {'eval_loss': 1.046757459640503,\n",
    " 'eval_accuracy': 0.6996526962969916,\n",
    " 'eval_precision': 0.6731825324243581,\n",
    " 'eval_recall': 0.6996526962969916,\n",
    " 'eval_f1': 0.6802165883555797,\n",
    " 'eval_kappa': 0.4964696291856796,\n",
    " 'eval_runtime': 160.8954,\n",
    " 'eval_samples_per_second': 264.855,\n",
    " 'eval_steps_per_second': 33.108}\n",
    "normal_eval = {'eval_loss': 0.8135748505592346,\n",
    " 'eval_accuracy': 0.7065518374243206,\n",
    " 'eval_precision': 0.668486859823052,\n",
    " 'eval_recall': 0.7065518374243206,\n",
    " 'eval_f1': 0.6745953287596376,\n",
    " 'eval_kappa': 0.46453323665763047,\n",
    " 'eval_runtime': 173.5557,\n",
    " 'eval_samples_per_second': 245.535,\n",
    " 'eval_steps_per_second': 30.693,\n",
    " 'epoch': 3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec7d6e6f-0ca3-4e76-89d9-869ef5c7d9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_kappa</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pruned</th>\n",
       "      <td>1.046757</td>\n",
       "      <td>0.699653</td>\n",
       "      <td>0.673183</td>\n",
       "      <td>0.699653</td>\n",
       "      <td>0.680217</td>\n",
       "      <td>0.496470</td>\n",
       "      <td>160.8954</td>\n",
       "      <td>264.855</td>\n",
       "      <td>33.108</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normal</th>\n",
       "      <td>0.813575</td>\n",
       "      <td>0.706552</td>\n",
       "      <td>0.668487</td>\n",
       "      <td>0.706552</td>\n",
       "      <td>0.674595</td>\n",
       "      <td>0.464533</td>\n",
       "      <td>173.5557</td>\n",
       "      <td>245.535</td>\n",
       "      <td>30.693</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        eval_loss  eval_accuracy  eval_precision  eval_recall   eval_f1  \\\n",
       "pruned   1.046757       0.699653        0.673183     0.699653  0.680217   \n",
       "normal   0.813575       0.706552        0.668487     0.706552  0.674595   \n",
       "\n",
       "        eval_kappa  eval_runtime  eval_samples_per_second  \\\n",
       "pruned    0.496470      160.8954                  264.855   \n",
       "normal    0.464533      173.5557                  245.535   \n",
       "\n",
       "        eval_steps_per_second  epoch  \n",
       "pruned                 33.108    NaN  \n",
       "normal                 30.693    3.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([pruned_eval, normal_eval], index=[\"pruned\",\"normal\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbb8dad0-6ee7-41b2-b9c9-3607450aa4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pruned</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>1.046757</td>\n",
       "      <td>0.813575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.699653</td>\n",
       "      <td>0.706552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_precision</th>\n",
       "      <td>0.673183</td>\n",
       "      <td>0.668487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_recall</th>\n",
       "      <td>0.699653</td>\n",
       "      <td>0.706552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_f1</th>\n",
       "      <td>0.680217</td>\n",
       "      <td>0.674595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_kappa</th>\n",
       "      <td>0.496470</td>\n",
       "      <td>0.464533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pruned    normal\n",
       "eval_loss       1.046757  0.813575\n",
       "eval_accuracy   0.699653  0.706552\n",
       "eval_precision  0.673183  0.668487\n",
       "eval_recall     0.699653  0.706552\n",
       "eval_f1         0.680217  0.674595\n",
       "eval_kappa      0.496470  0.464533"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t = df.iloc[:, :-4].T\n",
    "df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc3e2a91-6464-4e4c-8c17-862bf7e1df44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_t.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fc0f5-5478-4fea-9dbe-7a94f32b98ec",
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class NoisyTrainingArguments(TrainingArguments):\n",
    "    temperature: float = field(default=2.0, metadata={\"help\": \"Temperature for the softmax temperature.\"})\n",
    "    alpha_ce: float = field(default=0.5, metadata={\"help\":\"Linear weight for the distillation loss. Must be >=0.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e3af9-8bcd-4011-bf15-1220430df048",
   "metadata": {},
   "source": [
    "class NoisyStudentTrainer(Trainer):\n",
    "    def __init__(self, teacher_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        \n",
    "        # move the teacher model to the same device as the student model\n",
    "        self._move_model_to_device(self.teacher_model, self.model.device)\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # extract required parameters from our subclassed training arguments\n",
    "        temperature = self.args.temperature\n",
    "        alpha_ce = self.args.alpha_ce\n",
    "        \n",
    "        print(inputs)\n",
    "        \n",
    "        student_outputs = model(**inputs)\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher_model(**inputs)\n",
    "        t_logits = teacher_output.logits\n",
    "        s_logits = student_outputs.logits\n",
    "        \n",
    "        attention_mask = inputs.get(\"attention_mask\")\n",
    "        \n",
    "        # sanity check\n",
    "        assert t_logits.size() == s_logits.size()\n",
    "        \n",
    "        # Kullback-Leibler Divergence loss (cross entropy)\n",
    "        self.ce_loss_func = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        \n",
    "        # compute KLDiv loss and multiply by alpha value\n",
    "        loss_ce = self.ce_loss_func(nn.functional.log_softmax(s_logits / temperature, dim=-1),\n",
    "                                    nn.functional.softmax(t_logits / temperature, dim=-1)) * (temperature ** 2)\n",
    "        loss = alpha_ce * loss_ce\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094efcef-233f-4844-83c0-af4a1b20a5fa",
   "metadata": {},
   "source": [
    "class NoisyRobertaConfig(RobertaConfig):\n",
    "    def __init__(self, layerdrop=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layerdrop = layerdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c2d76-65f9-4f70-a1b8-72ac7c62efd9",
   "metadata": {},
   "source": [
    "class NoisyRobertaEncoder(RobertaEncoder):\n",
    "    # override the forward pass. Essentially mostly the same as the original code.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.config.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "                past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    if use_cache:\n",
    "                        logger.warning(\n",
    "                            \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                        )\n",
    "                        use_cache = False\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(layer_module),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = layer_module(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                        past_key_value,\n",
    "                        output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "                if use_cache:\n",
    "                    next_decoder_cache += (layer_outputs[-1],)\n",
    "                \n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "    \n",
    "class NoisyRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config: NoisyRobertaConfig):\n",
    "        super().__init__(config)\n",
    "        self.roberta.encoder = NoisyRobertaEncoder(config)\n",
    "        self.roberta.post_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9cddc4-8117-4286-8372-90b76d3a2f2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "config = NoisyRobertaConfig(layerdrop=0, num_labels=num_labels, problem_type=\"multi_label_classification\", max_length=512)\n",
    "teacher_model = RobertaForSequenceClassification.from_pretrained(r\"models_gitignored/roberta-base-finetuned-sentence-classification/checkpoint-75756/\", num_labels=num_labels, problem_type=\"multi_label_classification\").to(device)\n",
    "model = NoisyRobertaForSequenceClassification(config).to(device)\n",
    "\n",
    "model.roberta.embeddings.position_embeddings.weight.requires_grad = False\n",
    "model.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be84f7-7462-491f-87e9-11e25d190694",
   "metadata": {},
   "source": [
    "teacher_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a8d88-7918-47de-9878-e945e814eef4",
   "metadata": {},
   "source": [
    "# REFERENCE LINKS: \n",
    "[HOW NICE A BLOG POST](https://www.philschmid.de/knowledge-distillation-bert-transformers) \\\n",
    "[HF FORUM POST DISTILLATION WITH TRAINER](https://discuss.huggingface.co/t/does-it-make-sense-to-train-distilbert-from-scratch-in-a-new-corpus/3503/2) \\\n",
    "[BART GITHUB SOURCE CODE CTRL+F LAYERDROP](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bart/modeling_bart.py) \\\n",
    "[ROBERTA GITHUB SOURCE CODE](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py) \\\n",
    "[DEVICE SIDE ASSERT TRIGGERED ERROR (TL;DR JUST RESTART KERNEL)](https://stackoverflow.com/questions/68166721/cuda-error-device-side-assert-triggered-on-colab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
