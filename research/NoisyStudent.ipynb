{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1089791b-0420-4fb1-9b29-8ab602e70eb9",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7f09c-20ed-4226-8bd1-9e5f81b82cb1",
   "metadata": {},
   "source": [
    "Ensure that huggingface is installed via pip and not conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88901be2-4b67-4597-9cd8-b30aebc90da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from transformers import RobertaTokenizerFast, AutoTokenizer\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification\n",
    "from transformers.models.roberta.modeling_roberta import RobertaEncoder\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25a0f3f-f192-408f-92c1-3ceef5b3b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_labels = 7\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66038dc1-bd02-4e48-af22-912902a210ab",
   "metadata": {},
   "source": [
    "Lets import the data labelled by the teacher model. It should be pickled and in the form of a pandas dataframe. We then format the dataframe before passing it into datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00147e68-378f-4cb4-8b3e-e4c6e09525c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 2019 a wave of anti-abortion laws swept thi...</td>\n",
       "      <td>[6.635360240936279, -2.915717124938965, -0.175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But these grabbed the public’s attention in a ...</td>\n",
       "      <td>[-0.4229026734828949, -1.9662591218948364, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Georgia banned abortion after about six weeks ...</td>\n",
       "      <td>[1.8596746921539309, -3.230958938598633, -0.38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ohio, Mississippi, Louisiana and Kentucky did ...</td>\n",
       "      <td>[0.3270220756530761, -2.367975234985352, -1.59...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama went the furthest, banning virtually a...</td>\n",
       "      <td>[6.683916568756104, -2.563556671142578, -0.671...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28359</th>\n",
       "      <td>Jim Justice of West Virginia, a Republican, sa...</td>\n",
       "      <td>[-0.8264712691307068, -2.1442904472351074, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28360</th>\n",
       "      <td>The Biden administration had hoped to avoid sh...</td>\n",
       "      <td>[3.8891327381134033, -3.152361392974853, 1.242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28361</th>\n",
       "      <td>Late last year, the White House persuaded Ariz...</td>\n",
       "      <td>[5.278595924377441, -2.9199347496032715, -0.32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28362</th>\n",
       "      <td>Yet administration officials are less concerne...</td>\n",
       "      <td>[-0.3672137260437011, -2.0636661052703857, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28363</th>\n",
       "      <td>Gene Sperling, who oversees the Biden administ...</td>\n",
       "      <td>[0.5287293195724487, -2.7884507179260254, 0.09...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28364 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      In 2019 a wave of anti-abortion laws swept thi...   \n",
       "1      But these grabbed the public’s attention in a ...   \n",
       "2      Georgia banned abortion after about six weeks ...   \n",
       "3      Ohio, Mississippi, Louisiana and Kentucky did ...   \n",
       "4      Alabama went the furthest, banning virtually a...   \n",
       "...                                                  ...   \n",
       "28359  Jim Justice of West Virginia, a Republican, sa...   \n",
       "28360  The Biden administration had hoped to avoid sh...   \n",
       "28361  Late last year, the White House persuaded Ariz...   \n",
       "28362  Yet administration officials are less concerne...   \n",
       "28363  Gene Sperling, who oversees the Biden administ...   \n",
       "\n",
       "                                                  labels  \n",
       "0      [6.635360240936279, -2.915717124938965, -0.175...  \n",
       "1      [-0.4229026734828949, -1.9662591218948364, 0.2...  \n",
       "2      [1.8596746921539309, -3.230958938598633, -0.38...  \n",
       "3      [0.3270220756530761, -2.367975234985352, -1.59...  \n",
       "4      [6.683916568756104, -2.563556671142578, -0.671...  \n",
       "...                                                  ...  \n",
       "28359  [-0.8264712691307068, -2.1442904472351074, -0....  \n",
       "28360  [3.8891327381134033, -3.152361392974853, 1.242...  \n",
       "28361  [5.278595924377441, -2.9199347496032715, -0.32...  \n",
       "28362  [-0.3672137260437011, -2.0636661052703857, 0.5...  \n",
       "28363  [0.5287293195724487, -2.7884507179260254, 0.09...  \n",
       "\n",
       "[28364 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_path = r\"teacher_labels_0.pkl\"\n",
    "#df = pd.read_pickle(data_path)\n",
    "data_path = r\"data/pseudolab.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "df[\"labels\"] = df.iloc[:,1:-1].apply(lambda x: x.to_list(), axis=1)\n",
    "df = df.drop(columns=df.columns[1:-1])\n",
    "df = df.rename(columns={\"sentence\":\"text\"})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f1257-49f0-48d9-8286-0c6978318c77",
   "metadata": {},
   "source": [
    "We load the df into a dataset, where the labels take the same form of the multi label classification problem, except with float values ranging from 0-1 instead. We don't have to customise the tokenizer to suit our needs as both models are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c6ae3a-21f7-4044-98e7-71103a1cfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model_checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_checkpoint,\n",
    "                                          problem_type=\"multi_label_classification\",\n",
    "                                          use_fast=True, max_length=512)\n",
    "\n",
    "#tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_model_checkpoint,\n",
    "#                                                 problem_type=\"multi_label_classification\",\n",
    "#                                                 use_fast=True)\n",
    "\n",
    "def tokenize_and_encode(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d92408f-41d2-4bca-8366-1dc46708a19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96039cfb8f043299b40229f854dad7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa22a473d7f9424691af331b06e8bc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 19854\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 8510\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "dataset = dataset.map(tokenize_and_encode, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "'''\n",
    "features = dataset.features.copy()\n",
    "features[\"labels\"] = Sequence(feature=Value(\"float32\", id=None), length=-1, id=None)\n",
    "dataset.cast_(features)\n",
    "'''\n",
    "\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe712bd-8399-4486-8c1c-3c260c7d4b6b",
   "metadata": {},
   "source": [
    "Throw in compute_metrics() to determine model performance at the end of every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60fac429-af22-4ada-8b89-8e2dad571daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric_acc = load_metric(\"accuracy\")\n",
    "    metric_prec = load_metric(\"precision\")\n",
    "    metric_recall = load_metric(\"recall\")\n",
    "    metric_f1 = load_metric(\"f1\")\n",
    "    \n",
    "    # the logits aren't technically logits since the model head is a sequence classification head so its actually softmaxed probabilities\n",
    "    student_preds, teacher_logits = eval_preds\n",
    "    # since the labels themselves are also logits, we assume the teacher model is superior and that the highest probability teacher logit is the label\n",
    "    labels = np.argmax(nn.functional.softmax(input=teacher_logits, dim=-1), axis=-1)\n",
    "    predictions = np.argmax(student_preds, axis=-1)\n",
    "    \n",
    "    acc = metric_acc.compute(predictions=predictions, references=labels)\n",
    "    prec = metric_prec.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    recall = metric_recall.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    kappa = cohen_kappa_score(predictions, labels)\n",
    "\n",
    "    return {\"accuracy\": acc['accuracy'], \"precision\": prec['precision'],\n",
    "            \"recall\": recall['recall'], \"f1\": f1['f1'], \"kappa\": kappa}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e82c5-feac-4bdd-a456-3e69265b6bee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TRAINER USING SOFT LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6744b4-072c-4bb9-8ed9-a7c860f6a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NoisyTrainingArguments(TrainingArguments):\n",
    "    temperature: float = field(default=2.0, metadata={\"help\": \"Temperature for the softmax temperature.\"})\n",
    "    alpha_ce: float = field(default=0.5, metadata={\"help\":\"Linear weight for the distillation loss. Must be >=0.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f087d0a5-8809-413d-ab7d-27fd14bfb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyStudentTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # extract required parameters from our subclassed training arguments\n",
    "        temperature = self.args.temperature\n",
    "        alpha_ce = self.args.alpha_ce\n",
    "        \n",
    "        # get the labels of the input\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # get the outputs of the model forward pass\n",
    "        '''\n",
    "        attention_mask = inputs.attention_mask\n",
    "        input_ids = inputs.input_ids\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        '''\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # sanity check\n",
    "        assert logits.size() == labels.size()\n",
    "        \n",
    "        # Kullback-Leibler Divergence loss (cross entropy)\n",
    "        self.ce_loss_func = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        \n",
    "        # compute KLDiv loss and multiply by alpha value\n",
    "        loss_ce = self.ce_loss_func(nn.functional.log_softmax(logits / temperature, dim=-1),\n",
    "                                    nn.functional.softmax(labels / temperature, dim=-1)) * (temperature ** 2)\n",
    "        loss = alpha_ce * loss_ce\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee783c36-4840-490c-a88e-1299d58bcc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaConfig(RobertaConfig):\n",
    "    def __init__(self, layerdrop=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layerdrop = layerdrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "398fe3b5-b9d9-4ad5-ab8c-1344412b925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaEncoder(RobertaEncoder):\n",
    "    # override the forward pass. Essentially mostly the same as the original code.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.config.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "                past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    if use_cache:\n",
    "                        logger.warning(\n",
    "                            \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                        )\n",
    "                        use_cache = False\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(layer_module),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = layer_module(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                        past_key_value,\n",
    "                        output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "                if use_cache:\n",
    "                    next_decoder_cache += (layer_outputs[-1],)\n",
    "                \n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "class NoisyRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config: NoisyRobertaConfig):\n",
    "        super().__init__(config)\n",
    "        self.roberta.encoder = NoisyRobertaEncoder(config)\n",
    "        self.roberta.post_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825cae0d-c16e-4245-b897-7bb2c144b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = NoisyRobertaConfig(layerdrop=0,\n",
    "                            num_labels=num_labels,\n",
    "                            problem_type=\"multi_label_classification\")\n",
    "\n",
    "model = NoisyRobertaForSequenceClassification(config)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.roberta.embeddings.position_embeddings.weight.requires_grad = False\n",
    "model.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1de4927-e6a9-4f4b-b2d2-a1c5692f40a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = NoisyTrainingArguments(\n",
    "    output_dir=\"models_gitignored/roberta_noisy/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,    \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True)\n",
    "\n",
    "trainer = NoisyStudentTrainer(model=model,\n",
    "                              args=args, \n",
    "                              tokenizer=tokenizer,\n",
    "#                              data_collator=data_collator,\n",
    "                              train_dataset=dataset[\"test\"], \n",
    "                              eval_dataset=dataset[\"test\"],\n",
    "                              compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c015b7f6-334d-4db1-a8aa-ea27f4a7bfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weipyn/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8510\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1065' max='3192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1065/3192 03:10 < 06:21, 5.57 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1064' max='1064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1064/1064 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8510\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7239/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1563\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2208\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2209\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2210\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2452\u001b[0m         \u001b[0;31m# Metrics!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall_preds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2455\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7239/3435930920.py\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstudent_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# since the labels themselves are also logits, we assume the teacher model is superior and that the highest probability teacher logit is the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1680\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1681\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6243cf9b-3646-46ce-8f3b-1a6e61bd42d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# TRAINER WITH TEACHER MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e88f73-3827-4bab-83d6-ee21689d6db9",
   "metadata": {},
   "source": [
    "Here we subclass the TrainingArguments to inject our own parameters required for soft label loss computation. We follow the same format as official huggingface implementation referencing [seq2seq training arguments](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args_seq2seq.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff526b7-d7da-4ed8-af0a-41054e000dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NoisyTrainingArguments(TrainingArguments):\n",
    "    temperature: float = field(default=2.0, metadata={\"help\": \"Temperature for the softmax temperature.\"})\n",
    "    alpha_ce: float = field(default=0.5, metadata={\"help\":\"Linear weight for the distillation loss. Must be >=0.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9b6bc-7f44-474b-a5dc-b9d508c5b68f",
   "metadata": {},
   "source": [
    "We subclass the trainer and define our own compute_loss for soft labels. Much of this is based off the original work of [distil models](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff82590-2b61-434a-ad3f-d77b84b9d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyStudentTrainer(Trainer):\n",
    "    def __init__(self, teacher_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        \n",
    "        # move the teacher model to the same device as the student model\n",
    "        self._move_model_to_device(self.teacher_model, self.model.device)\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # extract required parameters from our subclassed training arguments\n",
    "        temperature = self.args.temperature\n",
    "        alpha_ce = self.args.alpha_ce\n",
    "        \n",
    "        print(inputs)\n",
    "        \n",
    "        student_outputs = model(**inputs)\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher_model(**inputs)\n",
    "        t_logits = teacher_output.logits\n",
    "        s_logits = student_outputs.logits\n",
    "        \n",
    "        attention_mask = inputs.get(\"attention_mask\")\n",
    "        \n",
    "        # sanity check\n",
    "        assert t_logits.size() == s_logits.size()\n",
    "        \n",
    "        # Kullback-Leibler Divergence loss (cross entropy)\n",
    "        self.ce_loss_func = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        \n",
    "        # compute KLDiv loss and multiply by alpha value\n",
    "        loss_ce = self.ce_loss_func(nn.functional.log_softmax(s_logits / temperature, dim=-1),\n",
    "                                    nn.functional.softmax(t_logits / temperature, dim=-1)) * (temperature ** 2)\n",
    "        loss = alpha_ce * loss_ce\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a44ad-e1f1-469a-bfd8-7805abc45280",
   "metadata": {},
   "source": [
    "Subclass RobertaConfig to include our own parameters relevant to layerdrop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86972ebb-63a4-48fd-b0c0-0501bb262b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaConfig(RobertaConfig):\n",
    "    def __init__(self, layerdrop=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layerdrop = layerdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae600e3a-f78b-496a-9e5b-1d5f022793d7",
   "metadata": {},
   "source": [
    "Subclass the encoder layers in roberta and implement layerdrop. Similar to BERT, RobertaModel can behave as an encoder and decoder, so we only have to subclass the single RobertaEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3cf537-caae-4cc4-b0e6-6683445a8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaEncoder(RobertaEncoder):\n",
    "    # override the forward pass. Essentially mostly the same as the original code.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.config.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "                past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    if use_cache:\n",
    "                        logger.warning(\n",
    "                            \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                        )\n",
    "                        use_cache = False\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(layer_module),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = layer_module(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                        past_key_value,\n",
    "                        output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "                if use_cache:\n",
    "                    next_decoder_cache += (layer_outputs[-1],)\n",
    "                \n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "    \n",
    "class NoisyRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config: NoisyRobertaConfig):\n",
    "        super().__init__(config)\n",
    "        self.roberta.encoder = NoisyRobertaEncoder(config)\n",
    "        self.roberta.post_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694ad45-1efc-4887-8f1d-7584314a638c",
   "metadata": {},
   "source": [
    "We also freeze the position and token type weights in roBERTa, I don't know why but the original authors of the distil models did that and I assume there's a good reason for doing so. The output of model is quite large so it is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6c1f6-36d9-424f-b845-0b08d6f7c64d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = NoisyRobertaConfig(layerdrop=0, num_labels=num_labels, problem_type=\"multi_label_classification\", max_length=512)\n",
    "teacher_model = RobertaForSequenceClassification.from_pretrained(r\"models_gitignored/roberta-base-finetuned-sentence-classification/checkpoint-75756/\", num_labels=num_labels, problem_type=\"multi_label_classification\").to(device)\n",
    "model = NoisyRobertaForSequenceClassification(config).to(device)\n",
    "\n",
    "model.roberta.embeddings.position_embeddings.weight.requires_grad = False\n",
    "model.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d30f5-5c42-41d5-ac9d-e4bb25462ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d99385-792f-4af5-aa06-06dddee39e8d",
   "metadata": {},
   "source": [
    "We prayge that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e54fe9-9ba0-41a8-9ec2-97c3a79e6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = NoisyTrainingArguments(\n",
    "    output_dir=\"models_gitignored/NoisyRoberta/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,    \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True)\n",
    "\n",
    "trainer = NoisyStudentTrainer(model=model,\n",
    "                              teacher_model=teacher_model,\n",
    "                              args=args, \n",
    "                              tokenizer=tokenizer,\n",
    "#                              data_collator=data_collator,\n",
    "                              train_dataset=dataset[\"train\"], \n",
    "                              eval_dataset=dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35da75c-bb6e-42fc-aba5-aed7146f3ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a8d88-7918-47de-9878-e945e814eef4",
   "metadata": {},
   "source": [
    "REFERENCE LINKS: \\\n",
    "[HOW NICE A BLOG POST](https://www.philschmid.de/knowledge-distillation-bert-transformers) \\\n",
    "[HF FORUM POST DISTILLATION WITH TRAINER](https://discuss.huggingface.co/t/does-it-make-sense-to-train-distilbert-from-scratch-in-a-new-corpus/3503/2) \\\n",
    "[BART GITHUB SOURCE CODE CTRL+F LAYERDROP](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bart/modeling_bart.py) \\\n",
    "[ROBERTA GITHUB SOURCE CODE](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py) \\\n",
    "[DEVICE SIDE ASSERT TRIGGERED ERROR (TL;DR JUST RESTART KERNEL)](https://stackoverflow.com/questions/68166721/cuda-error-device-side-assert-triggered-on-colab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
