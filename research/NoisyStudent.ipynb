{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88901be2-4b67-4597-9cd8-b30aebc90da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Trainer' from 'transformers' (C:\\Users\\weipy\\anaconda3\\envs\\cspy\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizerFast\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaConfig, RobertaModel\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Trainer' from 'transformers' (C:\\Users\\weipy\\anaconda3\\envs\\cspy\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import Trainer\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66038dc1-bd02-4e48-af22-912902a210ab",
   "metadata": {},
   "source": [
    "Lets import the data labelled by the teacher model. It should be pickled and in the form of a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00147e68-378f-4cb4-8b3e-e4c6e09525c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"teacher_labels_0.pkl\"\n",
    "df = pd.read_pickle(data_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562b257-82f1-4e5f-9ec3-5a380f1f7b52",
   "metadata": {},
   "source": [
    "We format the dataframe before passing it into datasets. \\\n",
    "textualsampleid | sentence | claimScore | positionScore | ... | concludingStatementScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d71f8-2245-4b38-9f74-5f2abe5860c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"labels\"] = df.iloc[:,2:].apply(lambda x: torch.tensor(x.to_list(), dtype=torch.float), axis=1)\n",
    "df = df.drop(columns=[\"textualsampleid\", \"leadScore\", \"positionScore\", \"evidenceScore\",\n",
    "                      \"claimScore\", \"concludingStatementScore\", \"counterclaimScore\", \"rebuttalScore\"])\n",
    "df = df.rename({\"sentence\":\"text\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f1257-49f0-48d9-8286-0c6978318c77",
   "metadata": {},
   "source": [
    "We load the df into a dataset, where the labels take the same form of the multi label classification problem, except with float values ranging from 0-1 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6ae3a-21f7-4044-98e7-71103a1cfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model_checkpoint = r\"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_checkpoint,\n",
    "                                          problem_type=\"multi_label_classification\")\n",
    "def tokenize_and_encode():\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92408f-41d2-4bca-8366-1dc46708a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df).set_format(\"torch\")\n",
    "dataset = dataset.map(tokenize_and_encode, batched=True, remove_columns=[\"text\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e88f73-3827-4bab-83d6-ee21689d6db9",
   "metadata": {},
   "source": [
    "Here we subclass the TrainingArguments to inject our own parameters required for soft label loss computation. We follow the same format as official huggingface implementation referencing [seq2seq training arguments](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args_seq2seq.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff526b7-d7da-4ed8-af0a-41054e000dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NoisyTrainingArguments(TrainingArguments):\n",
    "    temperature: float = field(default=2.0, metadata={\"help\": \"Temperature for the softmax temperature.\"})\n",
    "    alpha_ce: float = field(default=0.5, metadata={\"help\":\"Linear weight for the distillation loss. Must be >=0.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9b6bc-7f44-474b-a5dc-b9d508c5b68f",
   "metadata": {},
   "source": [
    "We subclass the trainer and define our own compute_loss for soft labels. Much of this is based off the original work of [distil models](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py). I specifically only implemeneted the MLM calculation of loss since roBERTa trains using a masked language modelling objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff82590-2b61-434a-ad3f-d77b84b9d705",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNoisyStudentTrainer\u001b[39;00m(\u001b[43mTrainer\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;66;03m# extract required parameters from our subclassed training arguments\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtemperature\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "class NoisyStudentTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # extract required parameters from our subclassed training arguments\n",
    "        temperature = self.args.temperature\n",
    "        alpha_ce = self.args.alpha_ce\n",
    "        \n",
    "        # get the labels of the input\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # get the outputs of the model forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # sanity check\n",
    "        assert logits.size() == labels.size()\n",
    "        \n",
    "        # Kullback-Leibler Divergence loss (cross entropy)\n",
    "        self.ce_loss_func = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        \n",
    "        # compute KLDiv loss and multiply by alpha value\n",
    "        loss_ce = self.ce_loss_func(nn.functional.log_softmax(s_logits_slct / temperaturec, dim=-1),\n",
    "                                    nn.functional.softmax(t_labels_slct / temperature, dim=-1)) * (temperature ** 2)\n",
    "        loss = alpha_ce * loss_ce\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a44ad-e1f1-469a-bfd8-7805abc45280",
   "metadata": {},
   "source": [
    "Subclass RobertaConfig to include our own parameters relevant to layerdrop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86972ebb-63a4-48fd-b0c0-0501bb262b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisyRobertaConfig(RobertaConfig):\n",
    "    def __init__(self, encoder_layerdrop=0.2, decoder_layerdrop=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder_layerdrop = encoder_layerdrop\n",
    "        self.decoder_layerdrop = decoder_layerdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae600e3a-f78b-496a-9e5b-1d5f022793d7",
   "metadata": {},
   "source": [
    "Subclass the encoder layers in roberta and implement layerdrop. Conveniently, RobertaModel can behave as an encoder and decoder, so we only have to subclass the single RobertaEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3cf537-caae-4cc4-b0e6-6683445a8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaEncoder(RobertaEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # specify layerdrop based on whether block acts as encoder or decoder\n",
    "        if self.config.is_decoder:\n",
    "            self.layerdrop = self.config.decoder_layerdrop\n",
    "        else:\n",
    "            self.layerdrop = self.config.encoder_layerdrop\n",
    "\n",
    "    # override the forward pass. Essentially mostly the same as the original code.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "                past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    if use_cache:\n",
    "                        logger.warning(\n",
    "                            \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                        )\n",
    "                        use_cache = False\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(layer_module),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = layer_module(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                        past_key_value,\n",
    "                        output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "                if use_cache:\n",
    "                    next_decoder_cache += (layer_outputs[-1],)\n",
    "                \n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "    \n",
    "def NoisyRoberta(RobertaModel):\n",
    "    def __init__(self, config: NoisyRobertaConfig):\n",
    "        super().__init__(config)\n",
    "        self.encoder = NoisyRobertaEncoder(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694ad45-1efc-4887-8f1d-7584314a638c",
   "metadata": {},
   "source": [
    "We also freeze the position and token type weights in roBERTa, I don't know why but the original authors of the distil models did that and I assume there's a good reason for doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6c1f6-36d9-424f-b845-0b08d6f7c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = NoisyRobertaConfig(encoder_layerdrop=0.2, decoder_layerdrop=0.2)\n",
    "model = NoisyRoberta(config)\n",
    "\n",
    "model.embeddings.position_embeddings.weight.requires_grad = False\n",
    "model.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28745d32-9a28-44f7-a534-d0e929401d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e54fe9-9ba0-41a8-9ec2-97c3a79e6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "args = NoisyTrainingArguments()\n",
    "trainer = NoisyStudentTrainer(model=model, args=args, tokenizer=tokenizer, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a8d88-7918-47de-9878-e945e814eef4",
   "metadata": {},
   "source": [
    "REFERENCE LINKS: \\\n",
    "[HOW NICE A BLOG POST](https://www.philschmid.de/knowledge-distillation-bert-transformers) \\\n",
    "[HF FORUM POST DISTILLATION WITH TRAINER](https://discuss.huggingface.co/t/does-it-make-sense-to-train-distilbert-from-scratch-in-a-new-corpus/3503/2) \\\n",
    "[MODELLING BART GITHUB SOURCE CODE CTRL+F LAYERDROP](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bart/modeling_bart.py)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
