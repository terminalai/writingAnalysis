{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88901be2-4b67-4597-9cd8-b30aebc90da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66038dc1-bd02-4e48-af22-912902a210ab",
   "metadata": {},
   "source": [
    "Lets import the data labelled by the teacher model. It should be pickled and in the form of a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00147e68-378f-4cb4-8b3e-e4c6e09525c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"teacher_labels_0.pkl\"\n",
    "df = pd.read_pickle(data_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562b257-82f1-4e5f-9ec3-5a380f1f7b52",
   "metadata": {},
   "source": [
    "We format the dataframe before passing it into datasets. \\\n",
    "textualsampleid | sentence | claimScore | positionScore | ... | concludingStatementScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d71f8-2245-4b38-9f74-5f2abe5860c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"labels\"] = df.iloc[:,2:].apply(lambda x: torch.tensor(x.to_list(), dtype=torch.float), axis=1)\n",
    "df = df.drop(columns=[\"textualsampleid\", \"leadScore\", \"positionScore\", \"evidenceScore\",\n",
    "                      \"claimScore\", \"concludingStatementScore\", \"counterclaimScore\", \"rebuttalScore\"])\n",
    "df = df.rename({\"sentence\":\"text\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f1257-49f0-48d9-8286-0c6978318c77",
   "metadata": {},
   "source": [
    "We load the df into a dataset, where the labels take the same form of the multi label classification problem, except with float values ranging from 0-1 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6ae3a-21f7-4044-98e7-71103a1cfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model_checkpoint = r\"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_checkpoint,\n",
    "                                          problem_type=\"multi_label_classification\")\n",
    "def tokenize_and_encode():\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92408f-41d2-4bca-8366-1dc46708a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df).set_format(\"torch\")\n",
    "dataset = dataset.map(tokenize_and_encode, batched=True, remove_columns=[\"text\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e88f73-3827-4bab-83d6-ee21689d6db9",
   "metadata": {},
   "source": [
    "Here we subclass the TrainingArguments to inject our own parameters required for soft label loss computation. We follow the same format as official huggingface documentation referencing [seq2seq training arguments](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args_seq2seq.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff526b7-d7da-4ed8-af0a-41054e000dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NoisyTrainingArguments(TrainingArguments):\n",
    "    temperature: float = field(default=2.0, metadata={\"help\": \"Temperature for the softmax temperature.\"})\n",
    "    alpha_ce: float = field(default=0.5, metadata={\"help\":\"Linear weight for the distillation loss. Must be >=0.\"})\n",
    "    alpha_mlm: float = field(default=0.5, metadata={\"help\":\"Linear weight for the CLM loss. Must be >=0.\"})\n",
    "    restrict_ce_to_mask: bool = field(default=False, metadata={\"help\":\"If true, compute the distillation loss only the [MLM] prediction distribution.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9b6bc-7f44-474b-a5dc-b9d508c5b68f",
   "metadata": {},
   "source": [
    "We subclass the trainer and define our own compute_loss for soft labels. Much of this is based off the original work of [distil models](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py). I specifically only implemeneted the MLM calculation of loss since roBERTa trains using a masked language modelling objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff82590-2b61-434a-ad3f-d77b84b9d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyStudentTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # extract required parameters from our subclassed training arguments\n",
    "        temperature = self.args.temperature\n",
    "        alpha_ce = self.args.alpha_ce\n",
    "        alpha_mlm = self.args.alpha_mlm\n",
    "        restrict_ce_to_mask = self.args.restrict_ce_to_mask\n",
    "        \n",
    "        # get the labels of the input\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # get the outputs of the model forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # sanity check\n",
    "        assert logits.size() == labels.size()\n",
    "        \n",
    "        # calculate (un)masked logits\n",
    "        if restrict_ce_to_mask:\n",
    "            mask = (lm_labels > -1).unsqueeze(-1).expand_as(logits)  # (bs, seq_length, voc_size)\n",
    "        else:\n",
    "            mask = attention_mask.unsqueeze(-1).expand_as(logits)  # (bs, seq_length, voc_size)\n",
    "        s_logits_slct = torch.masked_select(logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
    "        s_logits_slct = s_logits_slct.view(-1, logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
    "        t_logits_slct = torch.masked_select(labels, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
    "        t_logits_slct = t_logits_slct.view(-1, logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
    "        assert t_logits_slct.size() == s_logits_slct.size()\n",
    "        \n",
    "        # Kullback-Leibler Divergence loss (cross entropy)\n",
    "        self.ce_loss_func = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        # Cross Entropy Loss (masked/causal language modelling)\n",
    "        self.lm_loss_func = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        \n",
    "        # compute KLDiv loss and multiply by alpha value\n",
    "        loss_ce = self.ce_loss_func(nn.functional.log_softmax(s_logits_slct / temperaturec, dim=-1),\n",
    "                                    nn.functional.softmax(t_labels_slct / temperature, dim=-1)) * (temperature ** 2)\n",
    "        loss = alpha_ce * loss_ce\n",
    "        \n",
    "        loss_mlm = self.lm_loss_func(s_logits.view(-1, s_logits.size(-1)), lm_labels.view(-1))\n",
    "        loss += self.alpha_mlm * loss_mlm\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae600e3a-f78b-496a-9e5b-1d5f022793d7",
   "metadata": {},
   "source": [
    "Subclass roberta and implement layerdrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3cf537-caae-4cc4-b0e6-6683445a8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisyRobertaConfig(RobertaConfig):\n",
    "    def __init__(self, encoder_layerdrop=0.2, decoder_layerdrop=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder_layerdrop = encoder_layerdrop\n",
    "        self.decoder_layerdrop = decoder_layerdrop\n",
    "    \n",
    "    \n",
    "def NoisyRoberta(RobertaModel):\n",
    "    def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6c1f6-36d9-424f-b845-0b08d6f7c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig(encoder_layerdrop=0.2, decoder_layerdrop=0.2)\n",
    "model = BartForSequenceClassification(config)\n",
    "\n",
    "model.embeddings.position_embeddings.weight.requires_grad = False\n",
    "\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
