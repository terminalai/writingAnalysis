{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1089791b-0420-4fb1-9b29-8ab602e70eb9",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7f09c-20ed-4226-8bd1-9e5f81b82cb1",
   "metadata": {},
   "source": [
    "Ensure that huggingface is installed via pip and not conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88901be2-4b67-4597-9cd8-b30aebc90da0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/weipyn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from transformers import RobertaTokenizerFast, AutoTokenizer\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification\n",
    "from transformers.models.roberta.modeling_roberta import RobertaEncoder\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25a0f3f-f192-408f-92c1-3ceef5b3b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_labels = 7\n",
    "num_epochs = 5\n",
    "iterations = 3\n",
    "layerdrop = 0.2\n",
    "load_saved_teacher_labels = True\n",
    "init_teacher_model_path = r\"models_gitignored/roberta-base-finetuned-sentence-classification/checkpoint-75756\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66038dc1-bd02-4e48-af22-912902a210ab",
   "metadata": {},
   "source": [
    "Lets import the data to be labelled by the teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d197a73-485b-4824-8b08-b7c78be730fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Carrie Lam, the chief executive, announced tha...\n",
       "1         The airline expressed its condolences to the r...\n",
       "2         Russian forces had given a predawn deadline fo...\n",
       "3         President Biden’s nominee to the Supreme Court...\n",
       "4         Russian forces had given a predawn deadline fo...\n",
       "                                ...                        \n",
       "149656     if I'm not sure what college I want to attend...\n",
       "149657     seeking multiple opinions before making a har...\n",
       "149658    it is better to seek multiple opinions instead...\n",
       "149659    The impact of asking people to help you make a...\n",
       "149660    there are many other reasons one might want to...\n",
       "Length: 149661, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths = [r\"title-data/comprehensive_data.csv\", r\"datagen/nytimes/pseudo.csv\",\n",
    "              r\"data/feedback_prize/train.csv\"]\n",
    "texts = list()\n",
    "for path in data_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    texts.append(df.filter(regex=(r\".*text.*\")).iloc[:,0])\n",
    "texts_series = pd.concat(texts).reset_index(drop=True)\n",
    "texts_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f1257-49f0-48d9-8286-0c6978318c77",
   "metadata": {},
   "source": [
    "We pass the df into `pseudoLabel()` which returns a dataset, where the labels take the same form of the multi label classification problem, except with float values ranging from 0-1 instead. We don't have to customise the tokenizer to suit our needs as both models are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c6ae3a-21f7-4044-98e7-71103a1cfeb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768, padding_idx=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_model_checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_checkpoint,\n",
    "                                          problem_type=\"multi_label_classification\",\n",
    "                                          use_fast=True, max_length=128)\n",
    "\n",
    "init_teacher_model = RobertaForSequenceClassification.from_pretrained(init_teacher_model_path).to(device)\n",
    "init_teacher_model.resize_token_embeddings(len(tokenizer)) # this fixes the embedding problem\n",
    "#init_teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0899831c-e6bb-4fa2-8aa8-8c0056591ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def pseudoLabel(text, tokenizer, model):    \n",
    "    sentences = text.apply(sentence_tokenizer.tokenize).explode()\n",
    "    print(f\"{len(sentences)} sentences split\")\n",
    "    \n",
    "    def tokenize_and_encode(examples):\n",
    "        return tokenizer(examples['0'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(sentences))\n",
    "    dataset = dataset.map(tokenize_and_encode, batched=True, remove_columns=[\"0\"])\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    \n",
    "    def model_prediction(examples):\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=examples['input_ids'].to(model.device),\n",
    "                            attention_mask=examples['attention_mask'].to(model.device))\n",
    "        return {\"labels\":outputs.logits.cpu().detach().numpy()}\n",
    "    \n",
    "    dataset = dataset.map(model_prediction, batched=True, batch_size=100)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286103b3-b600-4164-8daa-012d313ffc9f",
   "metadata": {},
   "source": [
    "Pickle and dump information just in case we have to restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd9a00d7-ffb3-4a74-a4d7-3f4e6b5ffe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_saved_teacher_labels:\n",
    "    teacher_labelled_dataset = pseudoLabel(texts_series, tokenizer, init_teacher_model)\n",
    "    teacher_labelled_dataset\n",
    "    with open(r\"data/init_teacher_labelled_dataset\", \"ab\") as f:\n",
    "        pickle.dump(teacher_labelled_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10f28e-7936-445f-83cb-e4ff16b287c5",
   "metadata": {},
   "source": [
    "Loading from saved pickle file if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4813e3cc-51f5-4e11-a886-f609ba597958",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_saved_teacher_labels:\n",
    "    with open(r\"data/init_teacher_labelled_dataset\", \"rb\") as f:\n",
    "        teacher_labelled_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e442a-f263-4406-a351-dc85f28b30cc",
   "metadata": {},
   "source": [
    "Split the dataset. We realistically can't use all the data it takes 6 hours to train a single iteration of model and we do not have that kind of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d92408f-41d2-4bca-8366-1dc46708a19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 104402\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 44744\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_dataset = teacher_labelled_dataset.train_test_split(test_size=0.65)\n",
    "teacher_labelled_dataset = holdout_dataset['train'].train_test_split(test_size=0.3)\n",
    "teacher_labelled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe712bd-8399-4486-8c1c-3c260c7d4b6b",
   "metadata": {},
   "source": [
    "Throw in compute_metrics() to determine model performance at the end of every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fac429-af22-4ada-8b89-8e2dad571daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric_acc = load_metric(\"accuracy\")\n",
    "    metric_prec = load_metric(\"precision\")\n",
    "    metric_recall = load_metric(\"recall\")\n",
    "    metric_f1 = load_metric(\"f1\")\n",
    "    \n",
    "    # the logits aren't technically logits since the model head is a sequence classification head so its actually softmaxed probabilities\n",
    "    student_preds, teacher_logits = eval_preds\n",
    "    # since the labels themselves are also logits, we assume the teacher model is superior and that the highest probability teacher logit is the label\n",
    "    labels = np.argmax(nn.functional.softmax(input=torch.tensor(teacher_logits), dim=-1), axis=-1)\n",
    "    predictions = np.argmax(student_preds, axis=-1)\n",
    "    \n",
    "    acc = metric_acc.compute(predictions=predictions, references=labels)\n",
    "    prec = metric_prec.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    recall = metric_recall.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    kappa = cohen_kappa_score(predictions, labels)\n",
    "\n",
    "    return {\"accuracy\": acc['accuracy'], \"precision\": prec['precision'],\n",
    "            \"recall\": recall['recall'], \"f1\": f1['f1'], \"kappa\": kappa}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e82c5-feac-4bdd-a456-3e69265b6bee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TRAINER USING SOFT LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0178df-3338-420b-88c2-437e65838af2",
   "metadata": {},
   "source": [
    "Here we subclass the TrainingArguments to inject our own parameters required for soft label loss computation. We follow the same format as official huggingface implementation referencing [seq2seq training arguments](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args_seq2seq.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c6744b4-072c-4bb9-8ed9-a7c860f6a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NoisyTrainingArguments(TrainingArguments):\n",
    "    temperature: float = field(default=2.0, metadata={\"help\": \"Temperature for the softmax temperature.\"})\n",
    "    alpha_ce: float = field(default=0.5, metadata={\"help\":\"Linear weight for the distillation loss. Must be >=0.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4650457-ef98-4449-afa6-57aa90a0f0c6",
   "metadata": {},
   "source": [
    "We subclass the trainer and define our own compute_loss for soft labels. Much of this is based off the original work of [distil models](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/distiller.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f087d0a5-8809-413d-ab7d-27fd14bfb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyStudentTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # extract required parameters from our subclassed training arguments\n",
    "        temperature = self.args.temperature\n",
    "        alpha_ce = self.args.alpha_ce\n",
    "        \n",
    "        # get the labels of the input\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # get the outputs of the model forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # sanity check\n",
    "        assert logits.size() == labels.size()\n",
    "        \n",
    "        # Kullback-Leibler Divergence loss (cross entropy)\n",
    "        self.ce_loss_func = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        \n",
    "        # compute KLDiv loss and multiply by alpha value\n",
    "        loss_ce = self.ce_loss_func(nn.functional.log_softmax(logits / temperature, dim=-1),\n",
    "                                    nn.functional.softmax(labels / temperature, dim=-1)) * (temperature ** 2)\n",
    "        loss = alpha_ce * loss_ce\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a3303-1cad-4636-bd2f-3257967a126e",
   "metadata": {},
   "source": [
    "Subclass RobertaConfig to include our own parameters relevant to layerdrop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee783c36-4840-490c-a88e-1299d58bcc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaConfig(RobertaConfig):\n",
    "    def __init__(self, layerdrop=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layerdrop = layerdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3588a6-0d7a-4874-ab9c-1579c7ebfcf3",
   "metadata": {},
   "source": [
    "Subclass the encoder layers in roberta and implement layerdrop. Similar to BERT, RobertaModel can behave as an encoder and decoder, so we only have to subclass the single RobertaEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "398fe3b5-b9d9-4ad5-ab8c-1344412b925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRobertaEncoder(RobertaEncoder):\n",
    "    # override the forward pass. Essentially mostly the same as the original code.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.config.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "                past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    if use_cache:\n",
    "                        logger.warning(\n",
    "                            \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                        )\n",
    "                        use_cache = False\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(layer_module),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = layer_module(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                        past_key_value,\n",
    "                        output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "                if use_cache:\n",
    "                    next_decoder_cache += (layer_outputs[-1],)\n",
    "                \n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "class NoisyRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config: NoisyRobertaConfig):\n",
    "        super().__init__(config)\n",
    "        self.roberta.encoder = NoisyRobertaEncoder(config)\n",
    "        self.roberta.post_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd27bf3-af5f-419c-82d4-d74f291f4f6b",
   "metadata": {},
   "source": [
    "We also freeze the position and token type weights in roBERTa, I don't know why but the original authors of the distil models did that and I assume there's a good reason for doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cae0d-c16e-4245-b897-7bb2c144b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `NoisyRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__.\n",
      "/home/weipyn/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 104402\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65255\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2795' max='65255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2795/65255 05:37 < 2:05:41, 8.28 it/s, Epoch 0.21/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    # train the model\n",
    "    config = NoisyRobertaConfig(layerdrop=layerdrop,\n",
    "                                num_labels=num_labels,\n",
    "                                problem_type=\"multi_label_classification\")\n",
    "\n",
    "    model = NoisyRobertaForSequenceClassification(config).to(device)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer)) # this fixes the embedding problem\n",
    "    model.roberta.embeddings.position_embeddings.weight.requires_grad = False\n",
    "    model.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    save_dir = f\"roberta_noisy_layerdrop{layerdrop}_iter{i}\"\n",
    "    args = NoisyTrainingArguments(\n",
    "        output_dir=f\"models_gitignored/{save_dir}/\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,    \n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True)\n",
    "\n",
    "    trainer = NoisyStudentTrainer(model=model,\n",
    "                                  args=args, \n",
    "                                  tokenizer=tokenizer,\n",
    "                                  train_dataset=teacher_labelled_dataset[\"train\"], \n",
    "                                  eval_dataset=teacher_labelled_dataset[\"test\"],\n",
    "                                  compute_metrics=compute_metrics)\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    with open(f\"logs/roberta_noisy/{save_dir}.txt\", \"w\") as fout:\n",
    "        for obj in trainer.state.log_history:\n",
    "            print(obj, file=fout)\n",
    "    \n",
    "    # use the best model for pseudolabelling again\n",
    "    teacher_labelled_dataset = pseudoLabel(texts_series, tokenizer, trainer.model)\n",
    "    with open(r\"data/init_teacher_labelled_dataset\", \"ab\") as f:\n",
    "        pickle.dump(teacher_labelled_dataset, f)    \n",
    "    holdout_dataset = teacher_labelled_dataset.train_test_split(test_size=0.65)\n",
    "    teacher_labelled_dataset = holdout_dataset['train'].train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39f68b-ded4-4ba5-958f-5d1a8694914b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6243cf9b-3646-46ce-8f3b-1a6e61bd42d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# TRAINER WITH TEACHER MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fc0f5-5478-4fea-9dbe-7a94f32b98ec",
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class NoisyTrainingArguments(TrainingArguments):\n",
    "    temperature: float = field(default=2.0, metadata={\"help\": \"Temperature for the softmax temperature.\"})\n",
    "    alpha_ce: float = field(default=0.5, metadata={\"help\":\"Linear weight for the distillation loss. Must be >=0.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e3af9-8bcd-4011-bf15-1220430df048",
   "metadata": {},
   "source": [
    "class NoisyStudentTrainer(Trainer):\n",
    "    def __init__(self, teacher_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        \n",
    "        # move the teacher model to the same device as the student model\n",
    "        self._move_model_to_device(self.teacher_model, self.model.device)\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # extract required parameters from our subclassed training arguments\n",
    "        temperature = self.args.temperature\n",
    "        alpha_ce = self.args.alpha_ce\n",
    "        \n",
    "        print(inputs)\n",
    "        \n",
    "        student_outputs = model(**inputs)\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher_model(**inputs)\n",
    "        t_logits = teacher_output.logits\n",
    "        s_logits = student_outputs.logits\n",
    "        \n",
    "        attention_mask = inputs.get(\"attention_mask\")\n",
    "        \n",
    "        # sanity check\n",
    "        assert t_logits.size() == s_logits.size()\n",
    "        \n",
    "        # Kullback-Leibler Divergence loss (cross entropy)\n",
    "        self.ce_loss_func = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        \n",
    "        # compute KLDiv loss and multiply by alpha value\n",
    "        loss_ce = self.ce_loss_func(nn.functional.log_softmax(s_logits / temperature, dim=-1),\n",
    "                                    nn.functional.softmax(t_logits / temperature, dim=-1)) * (temperature ** 2)\n",
    "        loss = alpha_ce * loss_ce\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094efcef-233f-4844-83c0-af4a1b20a5fa",
   "metadata": {},
   "source": [
    "class NoisyRobertaConfig(RobertaConfig):\n",
    "    def __init__(self, layerdrop=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layerdrop = layerdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c2d76-65f9-4f70-a1b8-72ac7c62efd9",
   "metadata": {},
   "source": [
    "class NoisyRobertaEncoder(RobertaEncoder):\n",
    "    # override the forward pass. Essentially mostly the same as the original code.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.config.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "                past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    if use_cache:\n",
    "                        logger.warning(\n",
    "                            \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                        )\n",
    "                        use_cache = False\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(layer_module),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = layer_module(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                        past_key_value,\n",
    "                        output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "                if use_cache:\n",
    "                    next_decoder_cache += (layer_outputs[-1],)\n",
    "                \n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "    \n",
    "class NoisyRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config: NoisyRobertaConfig):\n",
    "        super().__init__(config)\n",
    "        self.roberta.encoder = NoisyRobertaEncoder(config)\n",
    "        self.roberta.post_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9cddc4-8117-4286-8372-90b76d3a2f2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "config = NoisyRobertaConfig(layerdrop=0, num_labels=num_labels, problem_type=\"multi_label_classification\", max_length=512)\n",
    "teacher_model = RobertaForSequenceClassification.from_pretrained(r\"models_gitignored/roberta-base-finetuned-sentence-classification/checkpoint-75756/\", num_labels=num_labels, problem_type=\"multi_label_classification\").to(device)\n",
    "model = NoisyRobertaForSequenceClassification(config).to(device)\n",
    "\n",
    "model.roberta.embeddings.position_embeddings.weight.requires_grad = False\n",
    "model.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n",
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be84f7-7462-491f-87e9-11e25d190694",
   "metadata": {},
   "source": [
    "teacher_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ec86b-5586-49d8-8fa0-c408c80a5715",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = NoisyTrainingArguments(\n",
    "    output_dir=\"models_gitignored/NoisyRoberta/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,    \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True)\n",
    "\n",
    "trainer = NoisyStudentTrainer(model=model,\n",
    "                              teacher_model=teacher_model,\n",
    "                              args=args, \n",
    "                              tokenizer=tokenizer,\n",
    "#                              data_collator=data_collator,\n",
    "                              train_dataset=dataset[\"train\"], \n",
    "                              eval_dataset=dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a07fba-ba3d-40c1-9281-cbf92fcb24d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a8d88-7918-47de-9878-e945e814eef4",
   "metadata": {},
   "source": [
    "REFERENCE LINKS: \\\n",
    "[HOW NICE A BLOG POST](https://www.philschmid.de/knowledge-distillation-bert-transformers) \\\n",
    "[HF FORUM POST DISTILLATION WITH TRAINER](https://discuss.huggingface.co/t/does-it-make-sense-to-train-distilbert-from-scratch-in-a-new-corpus/3503/2) \\\n",
    "[BART GITHUB SOURCE CODE CTRL+F LAYERDROP](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bart/modeling_bart.py) \\\n",
    "[ROBERTA GITHUB SOURCE CODE](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py) \\\n",
    "[DEVICE SIDE ASSERT TRIGGERED ERROR (TL;DR JUST RESTART KERNEL)](https://stackoverflow.com/questions/68166721/cuda-error-device-side-assert-triggered-on-colab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
