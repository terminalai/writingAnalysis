@online{feedbackprize,
    author = "Georgia State University and Learning Agency Lab",
    title = "Feedback Prize 2021 Dataset",
    url = "https://www.kaggle.com/competitions/feedback-prize-2021/data/"
}

@misc{noisystudent,
  doi = {10.48550/ARXIV.1911.04252},
  
  url = {https://arxiv.org/abs/1911.04252},
  
  author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Self-training with Noisy Student improves ImageNet classification},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{teacherstudent,
  doi = {10.48550/ARXIV.1912.13179},
  
  url = {https://arxiv.org/abs/1912.13179},
  
  author = {Abbasi, Sajjad and Hajabdollahi, Mohsen and Karimi, Nader and Samavi, Shadrokh},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Modeling Teacher-Student Techniques in Deep Neural Networks for Knowledge Distillation},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{TSembeddingdistillation,
  title     = {The Pupil Has Become the Master: Teacher-Student Model-Based Word Embedding Distillation with Ensemble Learning},
  author    = {Shin, Bonggun and Yang, Hao and Choi, Jinho D.},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {3439--3445},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/477},
  url       = {https://doi.org/10.24963/ijcai.2019/477},
}

@inproceedings{SSC,
	doi = {10.18653/v1/d19-1383},
  
	url = {https://doi.org/10.18653%2Fv1%2Fd19-1383},
  
	year = 2019,
	publisher = {Association for Computational Linguistics},
  
	author = {Arman Cohan and Iz Beltagy and Daniel King and Bhavana Dalvi and Dan Weld},
  
	title = {Pretrained Language Models for Sequential Sentence Classification},
  
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})}
}

@misc{KDberttospeech,
  doi = {10.48550/ARXIV.2108.02598},
  
  url = {https://arxiv.org/abs/2108.02598},
  
  author = {Jiang, Yidi and Sharma, Bidisha and Madhavi, Maulik and Li, Haizhou},
  
  keywords = {Computation and Language (cs.CL), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@book{kullback1997information,
  title={Information theory and statistics},
  author={Kullback, Solomon},
  year={1997},
  publisher={Courier Corporation}
}

@misc{stochasticdepth,
  doi = {10.48550/ARXIV.1603.09382},
  
  url = {https://arxiv.org/abs/1603.09382},
  
  author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Networks with Stochastic Depth},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{knowledgedistillation,
  doi = {10.48550/ARXIV.1503.02531},
  
  url = {https://arxiv.org/abs/1503.02531},
  
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Distilling the Knowledge in a Neural Network},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{bert,
  doi = {10.48550/ARXIV.1810.04805},
  
  url = {https://arxiv.org/abs/1810.04805},
  
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{distilbert,
  doi = {10.48550/ARXIV.1910.01108},
  
  url = {https://arxiv.org/abs/1910.01108},
  
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{distilgpt2,
    url = {https://huggingface.co/distilgpt2},
    organization = {Hugging Face},
    title = {DistilGPT2},
    year = {2019}

}
@misc{roberta,
  doi = {10.48550/ARXIV.1907.11692},
  
  url = {https://arxiv.org/abs/1907.11692},
  
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{transferlearningtext,
  doi = {10.48550/ARXIV.1910.10683},
  
  url = {https://arxiv.org/abs/1910.10683},
  
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{transformer,
  doi = {10.48550/ARXIV.1706.03762},
  
  url = {https://arxiv.org/abs/1706.03762},
  
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@Inbook{validation_early_stopping,
author="Prechelt, Lutz",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="Early Stopping --- But When?",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="53--67",
abstract="Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (``early stopping''). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using different 12 problems and 24 different network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4{\%} on average), but cost much more training time (here: about factor 4 longer on average).",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_5",
url="https://doi.org/10.1007/978-3-642-35289-8_5"
}
@misc{macrof1,
  doi = {10.48550/ARXIV.1911.03347},
  
  url = {https://arxiv.org/abs/1911.03347},
  
  author = {Opitz, Juri and Burst, Sebastian},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Macro F1 and Macro F1},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{kappa_score,
    author = {Jacob Cohen},
    title ={A Coefficient of Agreement for Nominal Scales},
    journal = {Educational and Psychological Measurement},
    volume = {20},
    number = {1},
    pages = {37-46},
    year = {1960},
    doi = {10.1177/001316446002000104},
    URL = {https://doi.org/10.1177/001316446002000104},
    eprint = {https://doi.org/10.1177/001316446002000104}
}

@misc{multiclassmetrics,
  doi = {10.48550/ARXIV.2008.05756},
  
  url = {https://arxiv.org/abs/2008.05756},
  
  author = {Grandini, Margherita and Bagli, Enrico and Visani, Giorgio},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Metrics for Multi-Class Classification: an Overview},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{layerdrop,
  doi = {10.48550/ARXIV.1909.11556},
  
  url = {https://arxiv.org/abs/1909.11556},
  
  author = {Fan, Angela and Grave, Edouard and Joulin, Armand},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Reducing Transformer Depth on Demand with Structured Dropout},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{https://doi.org/10.48550/arxiv.1910.13461,
  doi = {10.48550/ARXIV.1910.13461},
  
  url = {https://arxiv.org/abs/1910.13461},
  
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{ma2019nlpaug,
  title={NLP Augmentation},
  author={Edward Ma},
  howpublished={https://github.com/makcedward/nlpaug},
  year={2019}
}

@misc{nlpaugexample,
  title={Mapping of exercise logs to a database using Neural Networks and data augmentation techniques},
  author={Juan Uribe and Mandy Korpusik},
  organisation={Loyola Marymount University},
  howpublished={https://github.com/juanuribe28/research-f2020},
  year={2020-2021}
}

@misc{xlnet,
  doi = {10.48550/ARXIV.1906.08237},
  
  url = {https://arxiv.org/abs/1906.08237},
  
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{xlnet_gen,
  author = {Aman Rusia},
  title = {XLNet-Gen: XLNet for Generating Language},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/rusiaaman/XLNet-gen}},
  commit = {5a784f97edba2d167f1775f69505bee52673419d}
}

@misc{synonymaug,
  doi = {10.48550/ARXIV.1809.02079},
  
  url = {https://arxiv.org/abs/1809.02079},
  
  author = {Niu, Tong and Bansal, Mohit},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{contextual_embedding,
  doi = {10.48550/ARXIV.2003.02245},
  
  url = {https://arxiv.org/abs/2003.02245},
  
  author = {Kumar, Varun and Choudhary, Ashutosh and Cho, Eunah},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Data Augmentation using Pre-trained Transformer Models},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{contextual_sentence_embedding,
  doi = {10.48550/ARXIV.1707.07328},
  
  url = {https://arxiv.org/abs/1707.07328},
  
  author = {Jia, Robin and Liang, Percy},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adversarial Examples for Evaluating Reading Comprehension Systems},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{contextual_word_embedding,
  doi = {10.48550/ARXIV.1805.06201},
  
  url = {https://arxiv.org/abs/1805.06201},
  
  author = {Kobayashi, Sosuke},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
@inproceedings{ppdb,
  title={PPDB: The Paraphrase Database},
  author={Juri Ganitkevitch and Benjamin Van Durme and Chris Callison-Burch},
  booktitle={NAACL},
  year={2013}
}

@article{paraphrase,
  doi = {10.48550/ARXIV.1506.03487},
  
  url = {https://arxiv.org/abs/1506.03487},
  
  author = {Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen and Roth, Dan},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {From Paraphrase Database to Compositional Paraphrase Model and Back},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{wordnet,
author = {Miller, George A.},
title = {WordNet: A Lexical Database for English},
year = {1995},
issue_date = {Nov. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/219717.219748},
doi = {10.1145/219717.219748},
abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
journal = {Commun. ACM},
month = {nov},
pages = {39–41},
numpages = {3}
}
@book{wordnetbook,
  abstract = {WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets. The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.},
  added-at = {2017-11-01T11:46:20.000+0100},
  address = {Cambridge, MA},
  biburl = {https://www.bibsonomy.org/bibtex/28472b4f9d7f2bfc4a97ffd4a023facc6/flint63},
  editor = {Fellbaum, Christiane},
  file = {eBook:1900-99/Fellbaum1998.pdf:PDF;MIT Press Product Page:http\://mitpress.mit.edu/books/wordnet:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/026206197X/:URL},
  groups = {public},
  interhash = {42daa1681607dd1d3f3234c605d84ec3},
  intrahash = {8472b4f9d7f2bfc4a97ffd4a023facc6},
  isbn = {978-0-262-06197-1},
  keywords = {01821 101 mitpress book shelf ai language processing ontology lexicon},
  publisher = {MIT Press},
  series = {Language, Speech, and Communication},
  timestamp = {2018-04-16T11:51:58.000+0200},
  title = {WordNet: An Electronic Lexical Database},
  username = {flint63},
  year = 1998
}


@article{topicmodellingviz, title={Visualizing Topic Models}, volume={6}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/14321}, abstractNote={ &lt;p&gt; Managing large collections of documents is an important problem for many areas of science, industry, and culture. Probabilistic topic modeling offers a promising solution. Topic modeling is an unsupervised machine learning method that learns the underlying themes in a large collection of otherwise unorganized documents. This discovered structure summarizes and organizes the documents. However, topic models are high-level statistical tools&amp;mdash;a user must scrutinize numerical distributions to understand and explore their results. In this paper, we present a method for visualizing topic models. Our method creates a navigator of the documents, allowing users to explore the hidden structure that a topic model discovers. These browsing interfaces reveal meaningful patterns in a collection, helping end-users explore and understand its contents in new ways. We provide open source software of our method. &lt;/p&gt; }, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Chaney, Allison and Blei, David}, year={2021}, month={Aug.}, pages={419-422} }

@misc{snysubmethod,
  doi = {10.48550/ARXIV.2005.14424},
  
  url = {https://arxiv.org/abs/2005.14424},
  
  author = {Ye, Mao and Gong, Chengyue and Liu, Qiang},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Cryptography and Security (cs.CR), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{abstsumm,
  doi = {10.48550/ARXIV.2105.00824},
  
  url = {https://arxiv.org/abs/2105.00824},
  
  author = {Puspitaningrum, Diyah},
  
  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, H.3.1; I.2.7, 68P20 (Primary) 68T07, 68T50 (Secondary)},
  
  title = {A Survey of Recent Abstract Summarization Techniques},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{gpt2sentence,
  title={Sentence Augmentation for Language Translation Using GPT-2},
  author={Sawai, Ranto and Paik, Incheon and Kuwana, Ayato},
  journal={Electronics},
  volume={10},
  number={24},
  pages={3082},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{sentence_aug,
  title={Sequence-to-sequence pre-training with data augmentation for sentence rewriting},
  author={Zhang, Yi and Ge, Tao and Wei, Furu and Zhou, Ming and Sun, Xu},
  journal={arXiv preprint arXiv:1909.06002},
  year={2019}
}

@misc{resnet,
  doi = {10.48550/ARXIV.1512.03385},
  
  url = {https://arxiv.org/abs/1512.03385},
  
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Residual Learning for Image Recognition},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@article{efficientnet,
  doi = {10.48550/ARXIV.1905.11946},
  
  url = {https://arxiv.org/abs/1905.11946},
  
  author = {Tan, Mingxing and Le, Quoc V.},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{layerdrop2,
  doi = {10.48550/ARXIV.2010.08265},
  
  url = {https://arxiv.org/abs/2010.08265},
  
  author = {Wang, Qiang and Xiao, Tong and Zhu, Jingbo},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
